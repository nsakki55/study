# Castella book
章ごとの内容をざっくりまとめる。
## 3章
3章解説スライド  
解説っていうよりまとめに近い  
https://www.slideshare.net/shoichipincotaguchi/3-75873101  
Ridge, Lassoの詳しい内容。変数選択に関わる部分。  

## 4章
４章前半まつけんさん解説スライド   
https://www.slideshare.net/matsukenbook/4-63878216  
４章後半、まとめスライド  
https://speakerdeck.com/ysekky/castella-book-chap4?slide=21  
線形分離、ロジスティック回帰の内容。  

## 5章
５章まとめスライド  
https://www.slideshare.net/TakayukiUchiba1/5-76583678    
こっちのスライドの方が詳しい。  
https://www.slideshare.net/KotaMori/556-65129868?qid=ba92724c-fa24-4177-adbd-0ae66b65195e&v=&b=&from_search=2  
スプラインのお話。平滑化がキーワドな章 

## 6章
６章前半まとめスライド  
https://www.slideshare.net/eguchiakifumi/6-66699709  
カーネル平滑化の内容。  
局所領域で推定が柔軟にできるように着目する点に近い観測点だけをつかって回帰関数が滑らかになるようにモデルを作る方法。  
局所限定は観測点から着目する点からの距離に基づく重みを付与する重み関数＝カーネルKを介して実現できるらしい。  
局所回帰に特価したカーネルのことであり、SVMで用いられる高次元特徴空間での内積を計算するカーネルとは異なる。 
### 6-1
KNNの回帰曲線は、着目点に近い点の平均が回帰曲線になるものだった。近傍の全ての点に等しい重みをKNNでは付与するため、回帰曲線が波打つ不連続なものになる。  
着目点からの距離に応じて重みが減少するといい感じに滑らかになる。局所多項式回帰では、問題ごとに次数がきまる  
### 6-2
カーネル幅はいろいろある。平均化に使う窓の幅を変えると、窓が小さいと少数の近傍の点の平均となり、分散は相対的に大きくなり、期待値は真値に近づくのでバイアスは小さくなる。  

### 6-3
多次元における局所回帰。6−２までの内容を多次元に一般化した話。１次元平滑化では境界での当てはめに問題があり、多次元では局所回帰は有効ではなくなってくる問題がある。  

### 6-4
多次元における構造化局所回帰モデル。相互作用が存在する回帰関数のあてはめ。中でも係数変化モデルは重要な具体例としてあげられる。

### 6-5
局所的な尤度を考え、局所ロジスティック回帰を考える。

### 6-6
カーネル密度に関わる回帰、分類、関連手法の話  
・カーネル密度推定  
・カーネル密度分類木：ベイズの定理を用いた分類へ帰着させている。
・ナイーブベイス分類木:特徴空有間の次元が大きく、正確な密度推定が難しい場合に有効。特徴Xがすべて独立という強い仮定をかす。ナイーブベイズと一般化加法的モデルの関係と線形判別モデルとロジスティック回帰の関係は似ている。  

### 6-7
カーネル法は着目点の周りの局所領域で簡単なモデルを当てはめ、局所化はカーネルと観測値で与えられる重みから行われる。  
同型基底関数はカーネル関数を組み合わせるモデル。ガウスカーネルを用いた具体例。

### 6-8
複数のモデルを組み合わせた混合モデルの話。混合モデルのパラメータはEMアルゴリズムを用いて最適化される。

## 7章
### 7-2
予測値が連続値、分類のそれぞれのモデルに対する誤差関数について述べ、モデルの複雑さがますと、訓練データセットへ過適合して、テストデータへの汎化性能が下がるという一般的な話。  
### 7-3
バイアスと分散の数式上での存在を説明。リッジ回帰の場合では、推定バイアスが生じるが、分散は減少する。バイアスと分散のトレードオフについてふれている。  
### 7-4

