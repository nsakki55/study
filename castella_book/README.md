# 統計的学習の基礎

公式ページ  
https://web.stanford.edu/~hastie/ElemStatLearn/  
  
  
## 2章

平滑化スプライン  
https://logics-of-blue.com/%E5%B9%B3%E6%BB%91%E5%8C%96%E3%82%B9%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%A8%E5%8A%A0%E6%B3%95%E3%83%A2%E3%83%87%E3%83%AB/  

バイアスとバリアンスのトレードオフ  
https://pekochin.com/bias-variance/  

## 3章
3章解説スライド  
解説っていうよりまとめに近い  
https://www.slideshare.net/shoichipincotaguchi/3-75873101  

リッジ回帰と多重共線性の関係  
https://www.bananarian.net/entry/multico-ridge  

グループラッソのpython実装  
https://qiita.com/AnchorBlues/items/4e50d3b98a40c8b3086e  



## 4章
４章前半まつけんさん解説スライド   
https://www.slideshare.net/matsukenbook/4-63878216  
４章後半、まとめスライド  
https://speakerdeck.com/ysekky/castella-book-chap4?slide=21  
線形分離、ロジスティック回帰の内容。  

## 5章
５章まとめスライド  
https://www.slideshare.net/TakayukiUchiba1/5-76583678    
こっちのスライドの方が詳しい。  
https://www.slideshare.net/KotaMori/556-65129868?qid=ba92724c-fa24-4177-adbd-0ae66b65195e&v=&b=&from_search=2  
スプラインのお話。平滑化がキーワドな章 

## 6章
６章前半まとめスライド  
https://www.slideshare.net/eguchiakifumi/6-66699709  
カーネル平滑化の内容。  
局所領域で推定が柔軟にできるように着目する点に近い観測点だけをつかって回帰関数が滑らかになるようにモデルを作る方法。  
局所限定は観測点から着目する点からの距離に基づく重みを付与する重み関数＝カーネルKを介して実現できるらしい。  
局所回帰に特価したカーネルのことであり、SVMで用いられる高次元特徴空間での内積を計算するカーネルとは異なる。 
### 6-1
KNNの回帰曲線は、着目点に近い点の平均が回帰曲線になるものだった。近傍の全ての点に等しい重みをKNNでは付与するため、回帰曲線が波打つ不連続なものになる。  
着目点からの距離に応じて重みが減少するといい感じに滑らかになる。局所多項式回帰では、問題ごとに次数がきまる  
### 6-2
カーネル幅はいろいろある。平均化に使う窓の幅を変えると、窓が小さいと少数の近傍の点の平均となり、分散は相対的に大きくなり、期待値は真値に近づくのでバイアスは小さくなる。  

### 6-3
多次元における局所回帰。6−２までの内容を多次元に一般化した話。１次元平滑化では境界での当てはめに問題があり、多次元では局所回帰は有効ではなくなってくる問題がある。  　


### 6-4
多次元における構造化局所回帰モデル。相互作用が存在する回帰関数のあてはめ。中でも係数変化モデルは重要な具体例としてあげられる。

### 6-5
局所的な尤度を考え、局所ロジスティック回帰を考える。

### 6-6
カーネル密度に関わる回帰、分類、関連手法の話  
・カーネル密度推定  
・カーネル密度分類木：ベイズの定理を用いた分類へ帰着させている。
・ナイーブベイス分類木:特徴空有間の次元が大きく、正確な密度推定が難しい場合に有効。特徴Xがすべて独立という強い仮定をかす。ナイーブベイズと一般化加法的モデルの関係と線形判別モデルとロジスティック回帰の関係は似ている。  

### 6-7
カーネル法は着目点の周りの局所領域で簡単なモデルを当てはめ、局所化はカーネルと観測値で与えられる重みから行われる。  
同型基底関数はカーネル関数を組み合わせるモデル。ガウスカーネルを用いた具体例。

### 6-8
複数のモデルを組み合わせた混合モデルの話。混合モデルのパラメータはEMアルゴリズムを用いて最適化される。

## 7章

AICとBICについてのスライド。統計モデルの真の分布との比較を行うための情報量基準  
http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/bayes070.pdf  



７章後半解説。内容はCVやモデル選択についてなので、まだわかりやすい方ではある。  
https://www.slideshare.net/mattuyuya/7-79  
### 7-2
予測値が連続値、分類のそれぞれのモデルに対する誤差関数について述べ、モデルの複雑さがますと、訓練データセットへ過適合して、テストデータへの汎化性能が下がるという一般的な話。  
### 7-3
バイアスと分散の数式上での存在を説明。リッジ回帰の場合では、推定バイアスが生じるが、分散は減少する。バイアスと分散のトレードオフについてふれている。  
### 7-4
訓練標本外誤差と訓練標本内誤差の差を最善度として定義。  
### 7-5
赤池情報量規準AICを求め、モデル選択をおこなう。
### 7-6
有効パラメータ数の定義。有効自由度とも言われる。
### 7-7
ベイズ情報量規準BICの話。BIC最小化でモデルを選ぶと、事後確率を近似的に最大化するモデルを選ぶことと等価。  
BICによるモデル選択は、相対的なモデルの良さを評価することができる。  
### 7-8
最小記述長MDLによるモデル選択。モデルを符号化するシステムと考え、符号化後の記述量がもっとも小さくなるように最適化する.
### 7-9
モデルの複雑さを定義できるバプニック＝チェルボネンキス次元（VC次元）の話。関数クラスの要素である関数のうねり具合を評価することで複雑度を測る。  
各点にどのようなニ値ラベルを割り当てても、その関数クラスに含まれる関数（線形関数など）で完全に細分できるということ。  
構造化リスク最小化法SRMをもちいて、VC次元を利用したモデル選択が行える。  
### 7-10
交差分割検証CVの話。テストデータの誤差期待値がCVでは求められ、訓練集合を固定した時の条件付き誤差は推定できない。  
分割数を増やすと、近似的に真の期待予測誤差の不偏推定量となる。分割数を増やすと訓練データが似てくるので、分散が大きくなる。  
誤った予測変数の選択として、初めに目的変数との相関が大きい特徴量のみにふるいをかけたモデルでCVを行うと、真のテスト誤差とくらべ、極端に性能がいい結果がでてしまう。 正しくは、交差分割のK個のグループに分けた時、評価用のk番目の標本以外の標本を用いて、変数選択を行いCVを行う。  
### 7-11
ブートストラップ法：復元抽出で訓練集合から標本集合をランダムに取り出しそれぞれでモデルを作成して予測誤差、分散を求める。  
2クラス分類でのブートストラップ法の例を示している。

## 8章
8章前半の解説スライド  
https://speakerdeck.com/stakaya/tong-ji-de-xue-xi-falseji-chu-du-shu-hui-8zhang-model-inference-and-averaging-8-dot-4made
### 8-1
８章までの内容は最尤推定法が関わっていて、その関連性をのべる。ベイズ的推論手法にまで落としこんで説明を行なっている章になる。  
### 8-2
例としてBスプラインを基底関数として展開したモデルを考える。ブートストラップで50個のデータでサンプリングし、それぞれで予測値をだし、上下２．５％番目にあるデータを９５％信頼幅の推定値とする。   
パラメトリックブートストラップ：モデルを仮定。B回、予測結果に直接ノイズを加える操作を繰り返す。  
B→無限大で、パラメトリックブースティングの結果と最小二乗法の結果が一致する。
### 8-3
ベイズによる推論についての内容。ベイズ法は、データを得る前の事前的な知識を反映した事前分布を設定し、事後分布を計算する。  
ベイズ法は、他の手法とデータを見る前の不確実性を表すための事前分布を用いている点で異なり、データを見た後も事後分布の形で不確実性を残す点で異なる。  
事前分散τを無限大にすると、ガウスモデルは最尤推定とパラメトリックブートストラップ解析と一致する。  
### 8-4
ブートストラップとベイズが事前分散が無限大の場合で一致する理由を説明。   
ブートストラップ分布は無事前情報分布に対応している。よって、ブートストラップはお手軽な、ベイズ事後分布と考えられる。  
### 8-5
EMアルゴリズムのわかりやすい解説。  
https://www.slideshare.net/sotetsukoyamada/em-36315279  
最尤推定の問題を解くためのEMアルゴリズムの解説。  
二つのガウス分布の混合モデルの数値的に最大化するのが難しい尤度の問題を考える。  
尤度最大化は難しいが、未観測の隠れ変数を加えて標本を拡張する(データ拡張)ことで簡単にできる場合。   
EMアルゴリズムは、期待値(Expect)ステップ、最大化(Maximize)ステップからなる操作を収束するまで反復する。 
一般的なEMアルゴリズムでは、目的関数である尤度を直接最大化するのではなく。完全対数尤度となる関数を最大化することで、問題を簡単にしている。  
### 8-6
事後分布から標本抽出するためのマルコフ連鎖モンテカルロ法（MCMC法）の説明。  
まず、確率変数U_kの同時分布からの標本の抽出を行うギブス標本抽出を行う。  
ギブス標本抽出＝定常分布が真の同時分布であるようなマルコフ連鎖＝マルコフ連鎖モンテカルロ  
### 8-7
ブートストラップ標本で学習したモデルから予測を行うバギングの手法を説明。  
ランダムフォレストを例に挙げてバギングを説明している。バギングは決定木のような不安定な手法の分散を大きく抑えることができる。  
### 8-8
スタッキングの説明。スタッキングでは高い複雑さを持つモデルに不当な重みを割り当てることを避けている。  
各モデルの予測値を線形回帰させることで、最終的な予測値を求めている.  
### 8-9
バンピングの説明。ブートストラップ標本を生成し、モデをそれぞれの標本に当てはめ予測値を各点で求める。そして訓練データでの予測誤差を最小にするモデルを選ぶ。

## 9章
### 9-1
線形モデルではなく、一般的な加法的モデルを扱う章。  
一般加法的モデルでは後退当てはめ法により平滑化関数Sがもとまる。一般加法的ロジスティック回帰を例に扱っている。　  
スパムメールでの加法的ロジスティック回帰が非線形性を考慮することで線形ロジスティック回帰より性能をあげていることを示している。  
特徴量が多くなった場合、後退当てはめ法は使用できなくなる問題があるので、ブースティングのように前向きに段階的に推定を行う方が良い。  
### 9-2
決定木に関する解説  
https://dev.classmethod.jp/machine-learning/2017ad_20171211_dt-2/  
決定木を詳しく説明。２特徴量での決定木による分類は、鳥瞰図で表すことができる。
回帰木：分割された領域の平均値を予測値として最小二乗法で最適化する。
分類木：分割してた頂点の不純度に交差エントロピー、Gini係数、誤分類率をつかう。  
決定木のデメリットとして、分割が不安定であり分散が大きいことがあげられる。特徴量同士の加法的な関係性を捉えるのが難しい。  
### 9-3
抑制的規則導出方PRIMの説明。データの分類を箱を用いて行う手法。  
### 9-4
多変量適応的回帰スプラインMARSを説明した内容。
### 9-5
階層的エキスパート混合モデルHME。決定木の一種で、木による分類が決定的に行われるのではなく、確率的に行われる。  
線形、ロジスティック回帰モデルが角ノードに埋め込まれている
### 9-5
特徴の欠損の解説。いきなり欠損の話がでて前の話と関係がよくわからない。。。  
欠損が完全にランダムなものなのか、それともなんらかの意味のある配置をしているのかは、欠損を埋めるのに重要なポイントになる。  
なんらかの意味がある場合は、欠損そのものを特徴として扱うようにする。 
特徴に相関がある場合は、欠損部分を予測するモデルを作成し、予測値で埋めるのが推奨されている。  

## 10章
１０章後半解説スライド  
https://www.slideshare.net/mocchi_/10-72090741  
### 10-1
ブースティングは弱分類器の出力を組み合わせるため、バギングなど他の合議ベースの手法と似ているが、本質的には異なる。  
ブースティングはバギングと異なり、弱分類器の予測ごとに重み付けされて影響力の違いをだすようにしている。  
AdaBoostの手順：１、まず観測値の重みを初期化、２、重みを用いて分類器を学習し重み付き誤分類率を求め、弱学習器にかかる重みを求める。次の繰り返し演算に向けて各予測値に対する重みが更新される。３、最終的に各ステップの弱学習器の予測値の重みつき和を求める。 
### 10-2
ブースティングは弱学習器を基底関数とした加法的モデルとかんがえられる。
### 10-3
前向き段階的加法的モデリング。すでに追加されている基底関数のパラメータや係数を調節せず、新たな基底関数を順次追加することで、損失関数の最小化を行う方法。  
### 10-4
アダブーストが損失関数に指数損失関数を用いた段階的加法的モデリングに等価である。  
### 10-5
指数関数である理由を説明している。計算効率の面で指数損失は優れる。

### 10-6
分類に用いる損失規準はなんでも、負のマージンに対して正のマージンより思い罰則を加えるものである必要がある。  
回帰では、二乗誤差損失が指数損失に、絶対値損失が二項対数尤度に類似している。
二乗損失は大きな絶対値誤差を持つ観測値に対して極端に大きな重みを与えるため、ロバスト性がかなり低下する。  
### 10-7
ノーフリーランチ定理に関する話題。与えられた問題に対してどの手法が最良か、事前に予測できることはほとんどない。  
入力が数値、理論値、カテゴリ型変数になり混在し、カテゴリが複数の階層を含むことがある。  
予測変数と応答変数の分布が裾が長く、大きく歪んでいることが多い。  
データマイニングは予測結果より、説明性の高いモデルが求められる。NNはパターン認識には役立つが、データマイニングには役立たない（といわれている）。  
決定木は、高速に説明性の高いモデルを構築できるため、データマイニングに強い。種類のことなるデータを扱え、欠損、データのスケーリング、外れ値にロバストなど利点が多い  
木のブースティングは木の利点を残したまま、精度をあげられるため注目されている。  
### 10-8
spamデータに決定木ブースティングを用いた場合の、特徴量の貢献度をプロット。  
### 10-9
ブースティング木の詳しい説明。予測変数値の空間を、重複のない木の終端頂点で表現される領域Rに分割する。
最適化問題を、二つの部分に分割して簡略化する。  
・任意のRに対して、Rの領域内の平均を予測値rとする。  
・Rをみつける。一般に難しい。Rを見つけるために、予測値rの推定も必要になる。貪欲なトップダウン繰り返し分割アルゴリズムを用いる。  
ブースティング木が最適化するべき対象は木の次の領域と定数の数値最適化

### 10-10
勾配ブースティングの内部アルゴリズムを詳しく解説した内容。  
弱学習器、関数f(x)の予測値からなる損失Lをfに関して最小化させるのが目的。f(x)の構造を無視するとargmin L(f)が求めるf  
数値最適化のために、fを要素ベクトルhの和の順次更新を実施する。  
最急降下法(バッチ学習)では目的関数の勾配をとり、勾配方向に更新する方法＝勾配降下法  
最急降下法と勾配ブースティングは全く別の数値最適化手法。最急降下法は正しい最小解への到達が難しい。訓練データでのみ勾配が定義されているため、新しいデータへの汎化能力がない。  
各繰り返しで、勾配ができる限り負に近くような木を導き出すことで対処。  
### 10-11
ブースティング木の各木の最適な大きさを一般的な方法で独立に最適化した場合、各木が大きくなりすぎ、性能が低下し計算量が増加する問題を解決。  
単純な方法として、全ての木を同じ大きさJに固定して、Jを調整する方法が提案。  
経験的にJ>１０が必要になることはほぼなく、4~8 でうまくいく。  
### 10-12
勾配ブースティングでの正則化の説明。  
主にブースティングの繰り替えし回数Mをコントロールすることが目的となる。繰り替えし回数Mを大きくするほど、訓練データでの損失は減少させられる。  
NNのearly stopping のように、適切なMを求める必要がある。  
・縮小法：各木の貢献度を0<μ<1の倍率でかけあわせる＝Ridge, Lassoのような正則化の役割。パラメータμはブースティングの学習率を制御している。  
μが大きければ、Mは小さくなり、トレードオフの関係にある。  
・部分標本化：学習に用いるデータを非復元抽出により選び、バギングのような役割をはたす。計算時間の短縮のみならず、より正確なモデル構築が可能となる。  

### 10-13
決定木単体は二次元図でモデル全体の構造を完全に把握できたが、ブースティング木は木の線形組み合わせのため別の手法で説明する必要がある。  
多くの場合、ほんの一部の特徴が予測に決定的な影響を与える。相対的な重要度を学習するのが有効。   
単体の木の場合は二乗関連度によって重要度を決定できる。  
f(x)上のXcの影響（平均）を計算した後のXsの影響を示す部分依存関数を考える。  
### 10-14
いくつかのデータセットに対して、様々な損失関数を用いた勾配ブースティングの具体例  
特徴量の重要度は高いのに、目的変数との相互作用（主効果）は小さい場合は、他の特徴量との相互作用が隠蔽されている可能性がある。

## 11章
ニューラルネットワークと題打ってあるが、今まで説明したモデルと同様に入力変数の線形結合を導出特徴量としてもとめ、目的変数を導出特徴量の非線形関数としてモデリングする意図がある。  
### 11-2
まずはp次元の入力ベクトルXから1次元の目的変数Yを推定する射影追跡回帰PPRを考える。f(X) = sum(g(wX))  
これは加法的モデルだが、入力Xに対してではなく、Xの線形結合である導出特徴量V＝wXに対する加法的モデルである。  
非線形関数の数Mを任意に大きくとり、非線形関数gを適切に選択すれば、射影追跡回帰モデルは任意の精度で近似することが可能。  
しかし、モデルの解釈が困難になる。M=1の場合は単一指標モデルと呼ばれ、線形回帰モデルをわずかに一般化したものとして扱うことができる.  
損失関数に二乗誤差を用い、関数gと方向ベクトルの推定を収束するまでガウス＝ニュートン法などの最適化法を繰り返す。  
射影追跡回帰モデルは提案当時は計算量が追いつかなかったため、あまり注目されなかった。  

### 11-3
バニラ型ニューラルネットワークを扱う。単層誤差逆伝搬ネットワーク、単相パーセプトロンと呼ばれる。  
世間で言われている説明を俗説として、ただの非線形統計モデルとして扱っている。  
中間層の活性化関数をシグモイド関数を用いており、動径基底関数RBFを用いた場合は、動径基底関数ネットワークと呼ばれる。  
導出特徴量Zは入力変数Xの基底展開とみなせるため、ニューラルネットワークは標準的な線形モデルとみなせる。  
今までの基底展開とことなるのは、基底関数のパラメータがデータから学習される点。  
活性化関数が恒等関数の場合は、モデル全体が線形モデルになる。よって、非線形拡張を行っているにすぎないと考えられる。   
ニューラルネットワークの初期段階では、脳の神経細胞の発火をもしていたため、ステップ関数を用いていたが、閾値での不連続性が最適化の妨げにしかならなかったので、シグモイド関数のような閾値周辺でより滑らかな特性をもつ関数に帰られた。

### 11-4
誤差関数の最小化のために逆誤差伝搬が使われ、計算過程では各ユニットが保持する量を局所的に記憶すればよいという利点がある。  
デルタ則（逆伝搬）により重みの計算が行われ、二乗誤差の場合でも、交差エントリピーの場合でも同様の計算方法ですむ利点がある。   
バッチ学習は、逐次的に学習を行うオンライン学習が可能  

### 11-5
ニューラネットワークの主要な問題点をまとめている。  
・初期値  
もし荷重がほぼ０だと、シグモイド関数の実行領域でほぼ線形になり、ニューラルネットワークが近似的に線形モデルになってしまう。   
荷重の初期値には０付近のランダムな値が選ばれる。これにより学習の初めはほぼ線形モデルだが、学習により荷重が増えると非線形性が強くなる。  
完全に０になると、誤差関数の微分が０となり完全に対象形になるため荷重が更新されなくなる。逆に大きな値を持つと、解の収束が悪くなる。  
・過学習  
ニュラールネットワークは多くのパラメータを持つため過学習しやすい。early stoppingを推奨しているが、早すぎると線形モデルに近い状態で終了してしまう危険性もある
。  
荷重減衰（weight decay）として正則化項を誤差関数に含める方法がある。  
推定荷重のヒートマップをヒントン図と呼ぶ。荷重の増減の変化がわかる。  
・入力のスケーリング  
ニューラルネットワークは値な大きな特徴の影響を大きく受けるので、入力変数を標準正規化しておく必要がある。  
・隠れユニットと隠れ層の数  
一般的には隠れユニットの数は少なすぎるより多すぎる方がよい。少ないと、データに含まれる非線形性を吸収するのに十分な柔軟性をモデルに与えられない。  
隠れユニットが多すぎる場合は、正則化をして余分な荷重を０にしさえすれば大きな問題にならない。各隠れ層は入力変数の粒度の異なる特徴を抽出することができる。  
・複数の極小解  
最終的に得られる結果は初期荷重の選択に大きく依存する。複数の初期条件で学習したモデルをバギングによる方法で組み合わせるのが予測値を平均化して良い結果が得られる。  

### 11-6
ニュラールネットワークの学習に際して、設定しておくべきパラメータは荷重減衰パラメータと隠れユニット数。  
### 11-7
mnistの例。まさかの畳み込みネットワークの説明。 

## 12章
SVM の理論と実装をまとめたサイト  
https://logics-of-blue.com/svm-concept/  
線形決定境界の一般化で、クラスが重なり合い線形分離不可能な場合に拡張を行う。  
一般化した際の名称をSVMと呼ぶ。他の方法にフィッシャー線形判別分析LDAの一般化がある。  

### 12-2
サポートベクトル分類器：入力特徴空間で線形な境界を探索する。  
分離のための超平面と訓練データの点xとの符号付き距離を考える。  
この時、最適化するのはマージンMの最大化である。クラスが分類可能な場合は話が簡単になる。  
クラスが重なりあっている場合に対応するため、スラック変数を定義。このとき、一部の点がマージンの側より誤分類側に入ることも許す。  
SV分類器ではクラス境界から見て正しく分類されている点は境界の形成に大きな影響を与えない。  
線形判別分析の場合はクラス分布の共分散とクラス重心の位置によって判別境界が決まる。ロジスティック回帰はSVMと似ている。  
SV分類器の最適化は凸最適化問題とされ、ラグランジュ乗数を利用した二次計画問題に置き換えられる。  

### 12-3
SV分類器が入力特徴空間で線形な境界を探索するのに対して、多項式やスプラインなどの基底展開を使って特徴空間を拡大することで、より柔軟な分類が可能になる。   
基底関数h(x)が選択されると、入力特徴h(x) = (h1(x), h2(x)....)を使ってSV分類器を当てはめることで非線形関数を作り出すことができる。  
サポートベクトルマシンの分類器は基底展開の入力特徴の境界を見つけるのがもとのアイディアになっている。  
SVMの最適化問題では、基底関数を求める必要がなく、内積がわかればよい。  
その内積を直接計算せずに求められるのがカーネルトリックである。  

決定的な言い方はしていないが、SVMは次元の呪いをうける。そもそも計算量が多いので、大規模な特徴量は手に負えない。  
SVMの回帰問題への応用のためには許容誤差関数を定義して、予測値と実測値がずれていても、ずれはなく予測は当たっているとみなす予測誤差を考える。   
誤差εを超えていた場合に損失として計上する。  
VC次元の話がここでも。完璧に分類ができる最大のデータ数を表す。  

### 12-4
線形判別分析LDAの拡張を行っている内容。  
LDAの特徴：  
・新しい観測値は最も近い重心を持つクラスに分類される。  
・各クラスの観測値が共通の分散をもつ他変量ガウス分布に従うならLDAはベイズ分類器を推定する。  
・LDAによって作られる決定境界は線形  
・データに対して低次元表現を与えるので、直感的な理解に役立つ。  
・線形決定境界なのでクラスを適切に分類できない場合が多い。  

この章でのアプローチ  
・LDAの問題を線形回帰問題として整理する。線形回帰をより柔軟なノンパラメトリック回帰に一般化させる方法により、より柔軟は判別分析を行う。＝適応型判別分析FDA  
ノンパラメトリック回帰では基底関数によって拡大された予測変数の集合を線形回帰することにあり、FDAも同様に基底関数による入力変数をLDAすることにより、SVMと同じようなアプローチをたどることになる。  
・画像のように非常に多くの予測変数が存在する場合には係数に罰則を与えて空間領域で滑らかにしたり、まとまりがあるようにする。＝罰則付き判別分析PDA  
拡大した基底集合はしばしば大きすぎるのでSVMと同じように正則化が必要になる。これはFDAに適切な正則化を行った回帰として実現できる。  
・各クラスを異なる重心を二つ以上もつガウス分布の混同によってモデル化する。より複雑な決定境界を許容しつつ、LDA同様に部分空間への削減が可能となる。＝混合判別分析MDA  
### 12-5
変換が施された変数に対して線形回帰を用いることでLDAを行う方法を説明。  

### 12-6
FDAに対して罰則化を加えた正則化判別分析を説明。  
一般化されたLDAの手順  
・基底展開h(x)により予測変数Xを拡大  
・拡大された空間で罰則付きLDAを行う。マハラノビス距離を求める。  
・分類を行う部分空間に分解する。  

### 12-7
混合ガウス分布を用いてFDA、PDAを一般化する。  

## 13章
特徴と分類結果の関係の性質を捉えることはできないが、予測性能はいいブラックボックスのモデルをあつかう。  

### 13-2
プロトタイプ方についての内容。特徴空間における「点の集合」としてくれんデータを表し、分類したい新しい点xは最も近いプロトタイプのクラスへ分類される。  
近さはユークリッド距離により定義される。距離計算は訓練標本全体を標準化した後に行われる。  
各クラスの分布をうまく捉えられるようにプロトタイプが配置されていると、とても効果的な分類モデルとなる。が、幾つのプロトタイプをどこに配置するかが問題となる。  
k-meansの手法を詳しく述べた内容。  
k-meansの手法で、セントロイドをプロトタイプとする。  
学習ベクトル量子化LVQ：訓練点が正解クラスのプロトタイプを引きつけ、その他のプロトタイプを追い払うようにする。  
混合ガウス分布：各クラスタを一つのガウス密度分布によって表す。EMアルゴリズムにより最適化を行う。  

### 13-3
k最近傍分類木（KNN）についての説明。  
ただの記憶に基づくもので、当てはめるモデルを必要としない。  
分類したい点xが与えられると、xまでの距離が最も近いk個の訓練点を見つけ、多数決によって分類を行う。  

### 13-4
特徴量の次元が大きくなるとKNNの性能が低下する問題を解決しようとする内容。  

## 14章
教師なし学習についての章

### 14-2
相関ルール分析についての説明。  
データで頻出する変数の値の組み合わせを見つける。2値データでの利用が最も多い。= バスケット分析  
データは、変数Xには商品が購入されたら１、されなければ０というように値が入る。  
→特徴ベクトルXのプロトタイプ集合を見つける。＝最頻出検出、バンプ探索  
データの一般化表現のままだと問題が難しいので、表現を単純化させる。  
目的をXの空間でその大きさや、サポート集合に比べて確率が高い領を探すことにする。  

### 14-3
クラスタ分析の目的：オブジェクト同士を部分集合「クラスタ」にグループ化すること。  
K平均クラスタリング（K-means）の手順  
・各データ点に対してユークリッド距離が最も近いクラスタ中心を特定  
・各クラスタ中心をその中心が最も近いデータ点の平均で置換える。  
クラスタリングの手法では距離である非類似度尺度の選択が基礎になる。  
類似度行列、行、列にともにオブジェクトが対応し、各要素はオブジェクト同士の近さを表す。ほとんどのクラスタリングアルゴリズムでは、非負で対角要素が０の非類似度行列を過程している。  
属性に基づいた非類似度：  
多くの場合、データは観測値ごとに変数または属性の値が与えられている。類似度行列を求めるために、観測間の非類似度をペアごとに計算する必要がある。  
属性ごとの非類似度に最もよく用いられるのは二乗誤差。  
属性のタイプごとに類似度の尺度を変える。  
・量的変数：連続値実数値で表現されるため、二乗誤差、絶対値誤差、また相関が用いられることがある。  
・順序変数：順序値の決められた順序で、i - 1/2 /M によって変換した値を用いる。  
・カテゴリ型変数：順序がないカテゴリ変数の場合、値と値とがどの程度異なっているかという度合いを設定する必要がある。
データ中の属性には欠損が存在することが多くあり、その場合は非類似度を求める際に、省くというのが一般的なやり方。  
もしくは、欠損していない値の平均値、中央値をいれるか、欠損というカテゴリにする。  
クラスタリングは、組み合わせアルゴリズム、混合モデル、最頻値探索の三つの型に分類できる。  
・組み合わせアルゴリズム：最もよく使われるクラスタリングアルゴリズム。データを表現する確率モデルを用いず、各観測をグループに直接割り当てる。  
k-meansクラスタリング法は、ベクトル量子化として役立っている。  
クラス多数の決定方法には、クラスタ数の変化によるクラス内類似度の差分の減少をみる方法がある。が、すこしヒューリスティックなやり方。   
ギャップ統計量として、二つの曲線のギャップが最も大きい場所がクラスタ数と推定する。  
・階層的クラスタリング：探索クラスタ数を割り当てる必要がなく、グループ間の類似度を、２グループに含まれる観測間の類似度をもとの設定する。  
階層的なクラスタを作成でき、各階層のクラスタはその一つ下の階層クラスタを併合することで作成される。  

・凝集型クラスタリング：各観測が一つのクラスタを構成し、そこから各ステップで最も近い二つのクラスタを一つのクラスタに併合していく。よって、二つのクラスタ間の非類似度を定義する必要がある。

### 14-5
主成成分分析PCAを互いに無相関な分散の大きさ順に主成分を求める手法としたが、この章では主成分をp次元実数空間中のN個の点を近似する線形多様体として表現している。
また、非線形への一般化も行っている。  
p次元空間中のデータ集合の主成分は、そのデータに対する回数がqの最適な線形近似を与える。  
線形近似したモデルの最小二乗規模によるデータのあてはめは、再構成誤差最小化に相当。  
特異点分解SVDの説明がある。行列XをX=UDVに分解する。  
U,Vは直行行列、Dは対角行列。  
主成分は次元削減や圧縮として有効なツールとされている。mnistを例に圧縮をおこなっている。  
k-meansのような伝統的なクラスタリング手法では、データのグループ化のために球状、楕円状の軽量を用いるため、同心円上の非凸なクラスタに対してうまくいかない。  
この問題を解決する手法に、スペクトラルクラスタリングがある。標準的なクラスタリングを一般化したもの。  
カーネル主成分分析：PCAは共分散行列の固有ベクトルから得られ、データの分散が極大になる方向を与える。カーネル化を用いると、データを非線形変換して得られる特徴で構成される特徴空間中でPCAを行うのと等しい。  
データ行列Xの主成分変数Xは内積（グラム）行列から計算K=XXできる。  
カーネルPCAはカーネル行列Kを特徴の内積φ(x)φ(x)で構成される行列とみなし、その固有ベクトルを求める。  

### 14-6
非縁行列因子分解NMFの説明。  
特徴Xを二つの値が非負値の行列に分解することX = WH  
初期値に依存する問題があるが、注目されている。  
### 14-7
他変量データ中の潜在的な信号源を特定するための因子分析を説明した内容。  
特異点分解SVDには潜在変数を用いた表現法がある。  
独立成分分析ICAのモデルは潜在変数を表すSが互いに無相関ということではなく、統計的に独立であることを仮定。  

## 15章
ようやくランダムフォレストについての章、一章まるまるランダムフォレストについての内容になっている。  

### 15-1
バギング、ブートストラップ集約＝予測関数の分散を低減させる→分散の大きく、低バイアスの決定木に有効  
バギングでは同じ決定木をブートストラップ標本された訓練データに何回も当てはめて平均をとる。  
ブースティングでは、弱学習器の合議が時間発展していき、学習器の重み付き投票により予測される点でバギングと異なる。  
ランダムフォレストでは、バギングの改良。相関の低い木を集めて、結果の平均をとる。

### 15-2
なぜランダムフォレストが良い結果を出すのかを解説した内容.  
バギングの考え方：ノイズを多く含み、近似的にバイアスがないモデルの平均をとることで、分散を低減させる。   
木がバギングに理想的な理由：  
データの中の複雑な相互作用関係をとらえることができる。木を深くすれば、バイアスは相対的に低くなる。ノイズが多く含まれるので、平均をとることの効果が大きい  
バギングによって生成された木は同一の分布に従うので、木全体の平均の期待値は、個々の木の期待値に等しい。  
集めた木の相関の強さによって、分散の大きが低減できなくなり、平均化の効果が制限される。  
→ランダムフォレストの考えは、分散を大きくせずに、木の間の相関を低くすることにある。  
ブートストラップした標本で、特徴量をm個選ぶ。mの大きさが小さい＝使用する特徴量を少なくすると、個々の木の相関を低減できる。  
性能を見ると、ブースティング木の方がいいことがわかる。  

### 15-3
分類の場合は、複数の木の多数決で決め、回帰の場合は複数の木の予測の平均を単純にとる。  
特徴量重要度を、それぞれの木のそれぞれの分割でジニ変数などの分割規準値をどれだけ改善できたかによって決定できる。  
ブースティングはいくつかの変数を完全に無視するのに対して、分割変数の候補を選択する操作で、どの変数もランダムフォレストの中に含まれる可能性は増加する。  
ランダムフォレストが成長するとき、訓練データに対してN✖️Nの類似度行列を計算する。  
類似度からランダムフォレストから見て、どの観測が類似しているかがわかる。  
有用な特徴量が少ない場合、分割変数の数が小さいと、性能が悪い木が生まれる。  
ランダムフォレストは、ブースティングと異なり過学習をおこなさない。  

### 15-4
ランダムフォレストはリッジ回帰との相性がよい。線形モデルにおいて似た大きさの係数を持つ大量の変数がある時に有効なので、相関の強い変数の係数を互いに近づける効果がある。  
ランダムフォレストとKNNは決定境界が似ているらしい。  

## 16章
### 16-1
バギング、ブースティング、スタッキングはアンサンブル学習とみなされる。  
ノンパラメトリック回帰の学習に用いられるベイズ法も多数の候補モデルをそれらの事後確率で平均したものであるため、アンサンブル学習の一種としてみなされる。  
### 16-2
Ridge, Lassoの正則化の影響の違いを詳しく説明。  
Lassoは多重共線性の影響を受けやすい。

### 16-3
アンサンブル学習を、基底関数の辞書の和をモデルとして考える。  


## 17章
グラフィカルモデルの章。他の章との関連性がわかりにくいのでパス  
 
 
## 18章  














