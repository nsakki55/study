# Castella book
章ごとの内容をざっくりまとめる。
## 3章
3章解説スライド  
解説っていうよりまとめに近い  
https://www.slideshare.net/shoichipincotaguchi/3-75873101  
Ridge, Lassoの詳しい内容。変数選択に関わる部分。  

## 4章
４章前半まつけんさん解説スライド   
https://www.slideshare.net/matsukenbook/4-63878216  
４章後半、まとめスライド  
https://speakerdeck.com/ysekky/castella-book-chap4?slide=21  
線形分離、ロジスティック回帰の内容。  

## 5章
５章まとめスライド  
https://www.slideshare.net/TakayukiUchiba1/5-76583678    
こっちのスライドの方が詳しい。  
https://www.slideshare.net/KotaMori/556-65129868?qid=ba92724c-fa24-4177-adbd-0ae66b65195e&v=&b=&from_search=2  
スプラインのお話。平滑化がキーワドな章 

## 6章
６章前半まとめスライド  
https://www.slideshare.net/eguchiakifumi/6-66699709  
カーネル平滑化の内容。  
局所領域で推定が柔軟にできるように着目する点に近い観測点だけをつかって回帰関数が滑らかになるようにモデルを作る方法。  
局所限定は観測点から着目する点からの距離に基づく重みを付与する重み関数＝カーネルKを介して実現できるらしい。  
局所回帰に特価したカーネルのことであり、SVMで用いられる高次元特徴空間での内積を計算するカーネルとは異なる。 
### 6-1
KNNの回帰曲線は、着目点に近い点の平均が回帰曲線になるものだった。近傍の全ての点に等しい重みをKNNでは付与するため、回帰曲線が波打つ不連続なものになる。  
着目点からの距離に応じて重みが減少するといい感じに滑らかになる。局所多項式回帰では、問題ごとに次数がきまる  
### 6-2
カーネル幅はいろいろある。平均化に使う窓の幅を変えると、窓が小さいと少数の近傍の点の平均となり、分散は相対的に大きくなり、期待値は真値に近づくのでバイアスは小さくなる。  

### 6-3
多次元における局所回帰。6−２までの内容を多次元に一般化した話。１次元平滑化では境界での当てはめに問題があり、多次元では局所回帰は有効ではなくなってくる問題がある。  

### 6-4
多次元における構造化局所回帰モデル。相互作用が存在する回帰関数のあてはめ。中でも係数変化モデルは重要な具体例としてあげられる。

### 6-5
局所的な尤度を考え、局所ロジスティック回帰を考える。

### 6-6
カーネル密度に関わる回帰、分類、関連手法の話  
・カーネル密度推定  
・カーネル密度分類木：ベイズの定理を用いた分類へ帰着させている。
・ナイーブベイス分類木:特徴空有間の次元が大きく、正確な密度推定が難しい場合に有効。特徴Xがすべて独立という強い仮定をかす。ナイーブベイズと一般化加法的モデルの関係と線形判別モデルとロジスティック回帰の関係は似ている。  

### 6-7
カーネル法は着目点の周りの局所領域で簡単なモデルを当てはめ、局所化はカーネルと観測値で与えられる重みから行われる。  
同型基底関数はカーネル関数を組み合わせるモデル。ガウスカーネルを用いた具体例。

### 6-8
複数のモデルを組み合わせた混合モデルの話。混合モデルのパラメータはEMアルゴリズムを用いて最適化される。

## 7章
７章後半解説。内容はCVやモデル選択についてなので、まだわかりやすい方ではある。  
https://www.slideshare.net/mattuyuya/7-79  
### 7-2
予測値が連続値、分類のそれぞれのモデルに対する誤差関数について述べ、モデルの複雑さがますと、訓練データセットへ過適合して、テストデータへの汎化性能が下がるという一般的な話。  
### 7-3
バイアスと分散の数式上での存在を説明。リッジ回帰の場合では、推定バイアスが生じるが、分散は減少する。バイアスと分散のトレードオフについてふれている。  
### 7-4
訓練標本外誤差と訓練標本内誤差の差を最善度として定義。  
### 7-5
赤池情報量規準AICを求め、モデル選択をおこなう。
### 7-6
有効パラメータ数の定義。有効自由度とも言われる。
### 7-7
ベイズ情報量規準BICの話。BIC最小化でモデルを選ぶと、事後確率を近似的に最大化するモデルを選ぶことと等価。  
BICによるモデル選択は、相対的なモデルの良さを評価することができる。  
### 7-8
最小記述長MDLによるモデル選択。モデルを符号化するシステムと考え、符号化後の記述量がもっとも小さくなるように最適化する.
### 7-9
モデルの複雑さを定義できるバプニック＝チェルボネンキス次元（VC次元）の話。関数クラスの要素である関数のうねり具合を評価することで複雑度を測る。  
各点にどのようなニ値ラベルを割り当てても、その関数クラスに含まれる関数（線形関数など）で完全に細分できるということ。  
構造化リスク最小化法SRMをもちいて、VC次元を利用したモデル選択が行える。  
### 7-10
交差分割検証CVの話。テストデータの誤差期待値がCVでは求められ、訓練集合を固定した時の条件付き誤差は推定できない。  
分割数を増やすと、近似的に真の期待予測誤差の不偏推定量となる。分割数を増やすと訓練データが似てくるので、分散が大きくなる。  
誤った予測変数の選択として、初めに目的変数との相関が大きい特徴量のみにふるいをかけたモデルでCVを行うと、真のテスト誤差とくらべ、極端に性能がいい結果がでてしまう。 正しくは、交差分割のK個のグループに分けた時、評価用のk番目の標本以外の標本を用いて、変数選択を行いCVを行う。  
### 7-11
ブートストラップ法：復元抽出で訓練集合から標本集合をランダムに取り出しそれぞれでモデルを作成して予測誤差、分散を求める。  
2クラス分類でのブートストラップ法の例を示している。

## 8章
8章前半の解説スライド  
https://speakerdeck.com/stakaya/tong-ji-de-xue-xi-falseji-chu-du-shu-hui-8zhang-model-inference-and-averaging-8-dot-4made
### 8-1
８章までの内容は最尤推定法が関わっていて、その関連性をのべる。ベイズ的推論手法にまで落としこんで説明を行なっている章になる。  
### 8-2
例としてBスプラインを基底関数として展開したモデルを考える。ブートストラップで50個のデータでサンプリングし、それぞれで予測値をだし、上下２．５％番目にあるデータを９５％信頼幅の推定値とする。   
パラメトリックブートストラップ：モデルを仮定。B回、予測結果に直接ノイズを加える操作を繰り返す。  
B→無限大で、パラメトリックブースティングの結果と最小二乗法の結果が一致する。
### 8-3
ベイズによる推論についての内容。ベイズ法は、データを得る前の事前的な知識を反映した事前分布を設定し、事後分布を計算する。  
ベイズ法は、他の手法とデータを見る前の不確実性を表すための事前分布を用いている点で異なり、データを見た後も事後分布の形で不確実性を残す点で異なる。  
事前分散τを無限大にすると、ガウスモデルは最尤推定とパラメトリックブートストラップ解析と一致する。  
### 8-4
ブートストラップとベイズが事前分散が無限大の場合で一致する理由を説明。   
ブートストラップ分布は無事前情報分布に対応している。よって、ブートストラップはお手軽な、ベイズ事後分布と考えられる。  
### 8-5
EMアルゴリズムのわかりやすい解説。  
https://www.slideshare.net/sotetsukoyamada/em-36315279  
最尤推定の問題を解くためのEMアルゴリズムの解説。  
二つのガウス分布の混合モデルの数値的に最大化するのが難しい尤度の問題を考える。  
尤度最大化は難しいが、未観測の隠れ変数を加えて標本を拡張する(データ拡張)ことで簡単にできる場合。   
EMアルゴリズムは、期待値(Expect)ステップ、最大化(Maximize)ステップからなる操作を収束するまで反復する。 
一般的なEMアルゴリズムでは、目的関数である尤度を直接最大化するのではなく。完全対数尤度となる関数を最大化することで、問題を簡単にしている。  
### 8-6
事後分布から標本抽出するためのマルコフ連鎖モンテカルロ法（MCMC法）の説明。  
まず、確率変数U_kの同時分布からの標本の抽出を行うギブス標本抽出を行う。  
ギブス標本抽出＝定常分布が真の同時分布であるようなマルコフ連鎖＝マルコフ連鎖モンテカルロ  
### 8-7
ブートストラップ標本で学習したモデルから予測を行うバギングの手法を説明。  
ランダムフォレストを例に挙げてバギングを説明している。バギングは決定木のような不安定な手法の分散を大きく抑えることができる。  
### 8-8
スタッキングの説明。スタッキングでは高い複雑さを持つモデルに不当な重みを割り当てることを避けている。  
各モデルの予測値を線形回帰させることで、最終的な予測値を求めている.  
### 8-9
バンピングの説明。ブートストラップ標本を生成し、モデをそれぞれの標本に当てはめ予測値を各点で求める。そして訓練データでの予測誤差を最小にするモデルを選ぶ。

## 9章
### 9-1
線形モデルではなく、一般的な加法的モデルを扱う章。  
一般加法的モデルでは後退当てはめ法により平滑化関数Sがもとまる。一般加法的ロジスティック回帰を例に扱っている。　  
スパムメールでの加法的ロジスティック回帰が非線形性を考慮することで線形ロジスティック回帰より性能をあげていることを示している。  
特徴量が多くなった場合、後退当てはめ法は使用できなくなる問題があるので、ブースティングのように前向きに段階的に推定を行う方が良い。  
### 9-2
決定木に関する解説  
https://dev.classmethod.jp/machine-learning/2017ad_20171211_dt-2/  
決定木を詳しく説明。２特徴量での決定木による分類は、鳥瞰図で表すことができる。
回帰木：分割された領域の平均値を予測値として最小二乗法で最適化する。
分類木：分割してた頂点の不純度に交差エントロピー、Gini係数、誤分類率をつかう。  
決定木のデメリットとして、分割が不安定であり分散が大きいことがあげられる。特徴量同士の加法的な関係性を捉えるのが難しい。  
### 9-3
抑制的規則導出方PRIMの説明。データの分類を箱を用いて行う手法。  
### 9-4
多変量適応的回帰スプラインMARSを説明した内容。
### 9-5
階層的エキスパート混合モデルHME。決定木の一種で、木による分類が決定的に行われるのではなく、確率的に行われる。  
線形、ロジスティック回帰モデルが角ノードに埋め込まれている
### 9-5
特徴の欠損の解説。いきなり欠損の話がでて前の話と関係がよくわからない。。。  
欠損が完全にランダムなものなのか、それともなんらかの意味のある配置をしているのかは、欠損を埋めるのに重要なポイントになる。  
なんらかの意味がある場合は、欠損そのものを特徴として扱うようにする。 
特徴に相関がある場合は、欠損部分を予測するモデルを作成し、予測値で埋めるのが推奨されている。  

## 10章
１０章後半解説スライド  
https://www.slideshare.net/mocchi_/10-72090741  
### 10-1
ブースティングは弱分類器の出力を組み合わせるため、バギングなど他の合議ベースの手法と似ているが、本質的には異なる。  
ブースティングはバギングと異なり、弱分類器の予測ごとに重み付けされて影響力の違いをだすようにしている。  
AdaBoostの手順：１、まず観測値の重みを初期化、２、重みを用いて分類器を学習し重み付き誤分類率を求め、弱学習器にかかる重みを求める。次の繰り返し演算に向けて各予測値に対する重みが更新される。３、最終的に各ステップの弱学習器の予測値の重みつき和を求める。 
### 10-2
ブースティングは弱学習器を基底関数とした加法的モデルとかんがえられる。
### 10-3
前向き段階的加法的モデリング。すでに追加されている基底関数のパラメータや係数を調節せず、新たな基底関数を順次追加することで、損失関数の最小化を行う方法。  
### 10-4
アダブーストが損失関数に指数損失関数を用いた段階的加法的モデリングに等価である。  
### 10-5
指数関数である理由を説明している。計算効率の面で指数損失は優れる。






