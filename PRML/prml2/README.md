# 下巻まとめ

## 6章  
https://www.slideshare.net/KeisukeSugawara/slide0629  
https://www.slideshare.net/yukaraikemiya/6-15415589  
松尾研  
https://www.slideshare.net/matsuolab/prml6  

３、４章のパラメトリックモデルでは、訓練データは重みを学習したら破棄される。非線形、複雑なモデルに対応できない。  
カーネル関数：元の次元から特徴空間次元に移して内積をとる関数＝２変数の類似度を定めるようなもの  
内積をとる理由：  
特徴空間は高次元なため、直接計算できず、カーネル関数を通せば簡単に計算できる。多くの線形モデルは双対表現で表すと、カーネル関数が自然に現れる、  

### 6-1
線形回帰モデルの予測関数はカーネル関数を用いた双対表現で表せる。  
利点：全てがカーネル関数で表現されるので、特徴ベクトルφを明示的に考える必要がなくなる。  

### 6-2
関数kが有効なカーネル＝グアム行列Kが半正定値  
多項式カーネル、ガウスカーネル、生成モデルによるカーネル、シグモイドカーネル  

### 6-3
基底関数φは一般的には、RBF（動径基底関数）を使用する　 
RBFの理由：正確に目的変数の値を再現する関数補完、入力変数にノイズが含まれる場合の補完  
RBFは正規化されている。→全ての規定関数が小さい値を持つ領域をなくす。  

#### 6-3-1
訓練集合{x,t}の同時分布p(x,t)の推定にParzen推定法を用いる。  

### 6-4
ガウス過程についての節  
これまで：回帰のための非線形モデルからカーネル方を導いた  
これから；ベイズ的な設定でも自然とカーネルが現れることをみる  
ガウス過程とは、関数y(x)上の確率分布で{x1,x2,,,xn)に対して{y(x1), y(x2),,}の同時分布がガウス分布に従うこと  
ガウス過程の考え方：パラメータwの事前分布p(w)を使うのではなく、関数y(x)の事前分布p(y)を直接定義してしまう。  

#### 6-4-1
線形回帰モデルを再び考える  
wの事前分布を考え、その線形結合であるy自身もガウス分布に従う。平均と分散が求まれば、yの同時分布が定まる。  

#### 6-4-2
ガウス過程では、同時分布が平均、共分散といぅ二時の統計量で記述される。カーネル関数で計算される  
ガウス過程の利点：無限個の規定関数に対応  
欠点：O(N^3)の計算量を必要とする、逆行列の計算が必要。  

#### 6-4-3
ガウス過程による予測は共分散関数の選択に依存  
パラメトリックな関数族を考えて、そのパラメータθをデータから推測。  
超パラメータの学習方法：  
尤度関数p(t|θ)を評価、簡単な方法は対数尤度関数を最大化するθの点推定を行う。  
尤度関数の最大化は共役勾配法などの最適化アルゴリズムを用いる。  

#### 6-4-5
確率的な手法による分類：区間（0,1）に収まる事後分布を求める  
ガウス過程のモデルは予測が実数値全体での値をとる  
→ガウス過程の出力を非線形な活性化関数で変換し、ガウス過程を分類問題に適用できるようにする  
2クラス分類問題の場合：ロジスティックシグモイド関数で変換  
目的：予測分布p(tn+1|t)の決定  
事後分布をガウス分布による近似。  
近似方法：変分推論法、EP法、ラプラス近似  

#### 6-4-6
ラプラス近似についての節  
目的はp(a|t)のラプラス近似(ガウス分布に近似)して、積分をしやすくする  
ラプラス近似の流れ  
1.ラプラス近似の式を導出  
2.もとの確率分布のモードを求める。→ニュートンラフソン法  
3.モードにおいてヘッセ行列を評価  

#### 6-4-5
ニューラルネットワークの隠れユニット数Mを無限大に極限をとると、ガウス過程に近く。  





