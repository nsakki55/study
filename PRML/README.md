# PRMLのざっくりまとめ

## 1章
１章の内容まとめスライド  
https://www.slideshare.net/takushimiki/prml-52113785  
https://www.slideshare.net/matsuolab/prml1-78265686　　
機械学習全般の教師あり、教師なし、強化学習の大きな枠についての説明。  
演習問題の解答  
https://drive.google.com/drive/folders/0Bz9yuvZCp4qSZXB1MUpQSG9KQWs

### 1-1
簡単なsin(x)から生成したノイズいりの人工データから曲線フィッティングによる回帰を行う。  
観測データにはノイズが乗っており、与えられたxに対する目的変数tの値には不確実性がある。  
確率論は、不確実性を厳密に定量的に表現する。  
基底関数を多項式とする線形回帰を考える。  
線形モデルの係数値は予測値と目的変数の差を表す誤差関数（損失関数と同義？）を最小化して達成できる。  
単純で、1番使われている誤差関数は二乗話誤差。  
平均二乗平方根誤差RMSEの利点はNで割ることでサイズの異なるデータ集合を比較できる。目的変数tと同じ尺度（単位）であることが保証されること。  
回帰モデルの多項式の次数を増やすと、次数の増加に伴い、係数の値が増大し、訓練データに適合しすぎてしまう。  
最小二乗でモデルのパラメータを求めるのは、最尤推定での特別な場合に相当する。  
→過学習が最尤推定の一般的な性質である。  
過学習の問題を避けるために、ベイズ的アプローチが有効。  
正則化の概念について説明。  
二次の正則化＝Ridge回帰＝ニューラルネットワークの文脈で荷重減衰  

### 1-2
確率論の基礎的な概念の解説。  
確率の同時分布の対称性と、情報定理からベイズの定理が導かれる。  
事前確率：観測するより前にわかっている確率  
事後確率：一度事象が確認されてからベイズの定理から求められる、推定した事象の確率  
今まで直感的に使ってきた確率はランダムな繰り返しの頻度とみなされる、古典的確率、頻度主義的な確率解釈。
ベイズの定理を機械学習に当てはめるとすると  
p（w|X） = p(X|w)p(w) / p(X)  
パラメータの事後確率　＝　モデル・パラメータの事前分布 / 規格化定数  
事象xを確認した後に、wに関する不確実性を事後分布の形で評価する。  
不確実性を定量的に表現し、新たなデータで修正していく方法をベイズ的な確率解釈は実現可能。  
p(X|w)を尤度といい,尤度を最大化させる方法を最尤推定法という。  
ベイズの定理は  
事後確率 ∝ 尤度 × 事前確率  
最尤推定の気持ちとしては、データを生成する確率を最大にするパラメータがいいパラメータだろうという考え。  
最尤推定の問題点  
・尤度p(X|w)は厳密には確率ではない  
・単純に尤度を最大化すると過学習しやすい  
・モデル選択が難しい場合が多い。   
ベイズ的なアプローチの利点は、事前知識を、事前確率として自然にモデルに入れることができる点。  
公平に見えるコインを３回投げて毎回表がでたとしても、表がでる確率を頻度主義的な考えだと１になるのを防ぐことができる。  
マルコフ連鎖モンテカルロ法MCMC法のようなサンプリング法の開発がベイズの定理を実用化させてきた。  

#### 1-2-4
ガウス分布の性質についての内容。  
ガウス分布は、平均と分散によって定められ、全領域で積分すると１になるため確率分布の用件を満たす。  
ガウス分布での最頻値と平均は一致する。  
同一の正規分布から独立にデータがN個生成された場合のデータ集合の確率（尤度）は、正規分布の積で表される。  
最尤推定で求めたパラメータの期待値はE=N-1/N σ^2で、バイアスのある推定量  
データ点の数が増えればバイアスの影響はなくなる。データ量が多い理由の一つ。  

#### 1-2-5
最尤法を用いて曲線フィッティングを行う。  
ノイズが正規分布から発生していると考えると、モデルは正規分布の形で書くことができる。p(t|x,w,β) = N(t|y(x,w), β^-1)  
ここから全てのデータからの尤度を求めて、最尤推定法によりパラメータを求める。  
データに基づいた事後確率を最大にするパラメータ推定法を最大事後確率（MAP）推定という。  

#### 1-2-6
完全なベイズアプローチではwのすべての値に関して積分する必要がある。  
予測分布はガウス分布の形で与えられる。  

### 1-3
モデルの性能評価をまとめた内容  
交差分割検証:訓練データをs分割、訓練時間はs倍。  
情報量規準：赤池情報量規準AIC、ベイズ情報量規準BICを用いる。  

### 1-4
データをます目に分割すると、入力変数が増加するとマス目の数が指数関数的に増大する。  
多項式曲線フィッティングを複数個の入力変数に拡張した場合、より高次の多項式が必要となり、係数の数はべき乗に増える。  
N次元の球の体積は表面に近い薄皮部分に集中するという、幾何的直感と一致しない  
大きい異次元空有間に伴う困難全般のことを次元の呪いとよぶ  
実用では目的変数の変化を生じさせる方向は限られているため、意外と高次元でもなんとかなる。

### 1-5
クラス分類や、回帰予測の値の決定は誤識別率が最小になるように決定される。  
最大事後確率を
最大事後確率をもつクラスに決定される。  
識別率だけ上げると、問題になる分類もある。→損失関数を定めて、期待損失を最小化させる。  
決定問題を解く時の三つのアプローチ  

・生成モデル：　　
クラス事後確率p(C|x)、同時分布を推論する。出力の分布だけではなく、入力の分布もモデル化→モデルからのサンプリングで入力空間で人工データを生成できる。  
データの周辺分布p(x)を決めることができるため、低い確率をとる新しいデータ点をみつけ、外れ値検出、新規性検出に用いることができる。  
同時分布p(x,C)を推定するのにデータを大量に消費するため、事後確率p(C|x)のみ必要な場合は識別モデルを用いれば良い。  

・識別モデル：  
事後確率のみを推論して決定論でクラス割り当て。出力の分布をモデル化  

・識別関数：  
入力から直接クラスラベルに写像する関数f(x)をみつける。確率は用いいない。   

事後確率を求める利点：
訓練データの修正による事前確率の修正が容易、棄却オプションを変更可能、複数のモデルの結合が容易  

回帰の場合は、損失関数を二乗誤差とする。  
最適解は、xが与えられた下でのtの条件つき平均で、回帰関数といわれる。  

損失関数＝MSE＋目標データが持つノイズ  
と表され、損失関数の最初値はデータのノイズで決まる。  

### 1-6
離散確率変数xを観測した時の情報量 h(x) = -log{p(x)}
確率が低い事象が起こった時ほど大きな値をとる。
h(x)の単位はビット
情報量の平均をエントロピーと呼ぶ.

#### 1-6-1
真の分布p(x)をq(x)で近似する。q(x)でxの値を特定するのに追加で必要な情報量はKLダイバージェンスで求められる。  
KLダイバージェンス＝（p(x)のエントロピー）　ー　（q（x）のエントロピー)  
KL大バージェンンスの最小化＝尤度最大化  

## ２章
有名な確率分布やその特徴をまとめた章。  
ベイズ推論をについても触れる。  
パラメトリック：  
ガウス分布の平均、分散など、少数のパラメータにより確率分布が決定される  
パラメタの値を決める手段は必要になり、そのうちの一つが尤度関数の最適化となる。  
ベイズ主義の立場では、パラメータに事前分布を導入して、観測データが与えられた場合のパラメータの事後分布をベイズの定理を用いて計算する。  

ノンパラメトリック：少数のパラメタでは確率分布が決まらない。一応パラメタはあるが、モデルの複雑さを決めるために使われる  

### 2-1
ベルヌーイ分布：コイン投げによる確率分布  
０がでる確率μのみでモデルが決定される。μについて尤度関数を微分して解くと、μの最尤推定量が求まる。  
＝サンプル平均sum(x)/N たまたま３回表が出た時に、最尤推定では必ず表がでるという推定結果になる。  
二項分布：コイン投げにおける表のでる回数  

#### 2-1-1
コイン投げの例のように最尤推定では、サンプル数が少ないと過学習を起こしやすいため、事前分布を考えるベイズ主義的に扱う。  
妥当なモデルを事前分布に採用←妥当性は検証する必要あり  
ベルヌーイ分布のパラメタμの事前分布にベータ分布を用いる。←共役性のため  
逐次学習：  
新たなサンプルが追加されることで、尤度関数を更新し、事後分布が更新される方法  

### 2-2
ベルヌーイ分布、二項分布の他変数への拡張を行なっている。＝多項分布  
他変量の場合の対数尤度の最大化はラグランジュの未定乗数法を用いて行える。
多項分布のパラメタμkの事前分布としてディリクレ分布を採用  

### 2-3
ガウス分布についての内容。  
複数の確率変数の和の確率分布はガウス分布になる(中心極限定理)  
確率変数が互いに独立であるこが必要  
ガウス分布はマハラノビス距離を通して、xに依存する。  
ガウス分布は単峰形（極大値が一つ）という条件から、多峰形の分布をうまく近似できない。  
→潜在変数を導入することで解決。  

#### 2-3-1
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、条件付き分布p(xa|xb) もガウス分布に従う  
→どのように同時分布の断面を切ってもガウス分布
この証明を説明している。  
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、周辺分布p(xa), p(xb)もガウス分布に従う。  
やっている内容は、ベクトルxを二つの部分ベクトルx = (xa,xb)に分割したガウス分布p(x）を考え、条件付き分布p(xa|xb)と周辺分布p(xa)を求めている。  

#### 2-3-3
ガウス分布における変数で、ベイズの定理的理解をする節
ガウス周辺分布p(x)と平均がxの線型関数で共分散はxとは独立であるガウス条件付き分布p(y|x)を定義している。  
＝線型ガウスモデル  

#### 2-3-4
多変量ガウス分布から標本Xが得られた時、母集団の多変量ガウス分布のパラメタを最尤推定で推定できる。  
対数尤度関数は、ガウス分布の十分統計量にのみ依存している。  
最尤推定による平均は、データXの平均と一致し、共分散も通常の共分散の平均を最尤推定に置き換えたものになる。  

#### 2-3-5
ガウス分布の最尤推定において、逐次推定を行う。  
誤差信号の方向へ推定量を更新。  
Robbins-Monroアルゴリズム＝一般的な最尤推定問題を逐次的に解く方法。  

#### 2-3-6
ガウス分布における最尤推定の枠組みで、パラメータ上の事前分布を導入してベイズ主義的な扱いをする。  
観測データが生じる確率である尤度関数は、平均μの関数とみなせる。  
事前分布p(μ）にガウス分布を選べば、尤度関数の共役事前分布となる。  
平均を既知として、分散を推定する場合は共役事前分布ガンマ分布を用いる。  
平均も分散も未知として推定する場合は、事前分布に正規-ガンマ分布を考える  

#### 2-3-7
スチューデントのt分布についての説明。  
μ＝１でコーシー分布となる。  
平均は同じで、精度の異なるようなガウス分布を無限個足し合わせたもの。  
ガウス分布に比べ裾が広く、外れ値に対してロバストである。  
t分布に対する最尤推定会はEMアルゴリズムによって求まる、  

#### 2-3-8
ガウス分布に周期変数を導入する。  
周期変数の観測値の集合は曲座標表示可能。  
フォンミーゼス分布：確率変数、統計量を極座標表示することで、ガウス分布の周期変数への一般化を行なっている分布、  

#### 2-3-9
混合ガウス分布：ガウス分布を線形結合することでタータの分布を表現する  
線型結合する重みの係数と各分布の平均と共分散を調節すれば、任意の連続な密度関数を任意の精度で近似できる。  
混合係数は確率の条件を満たしており、k番目の混合要素を選択する事前確率と捉えられる。
事後確率p(k|x)＝負担率とする  
最尤推定は解析解が難しい→EMアルゴリズムの出番  

### 2-4
指数型分布族の全般的な性質について説明した内容  
→ベルヌーイ分布、多項分布、ガウス分布  



