# PRMLのざっくりまとめ
作者によるノートブック  
https://github.com/amber-kshz/PRML  

## 1章　
１章の内容まとめスライド  
https://www.slideshare.net/takushimiki/prml-52113785  
https://www.slideshare.net/matsuolab/prml1-78265686　　
機械学習全般の教師あり、教師なし、強化学習の大きな枠についての説明。  
演習問題の解答  
https://drive.google.com/drive/folders/0Bz9yuvZCp4qSZXB1MUpQSG9KQWs

### 1-1
簡単なsin(x)から生成したノイズいりの人工データから曲線フィッティングによる回帰を行う。  
観測データにはノイズが乗っており、与えられたxに対する目的変数tの値には不確実性がある。  
確率論は、不確実性を厳密に定量的に表現する。  
基底関数を多項式とする線形回帰を考える。  
線形モデルの係数値は予測値と目的変数の差を表す誤差関数（損失関数と同義？）を最小化して達成できる。  
単純で、1番使われている誤差関数は二乗話誤差。  
平均二乗平方根誤差RMSEの利点はNで割ることでサイズの異なるデータ集合を比較できる。目的変数tと同じ尺度（単位）であることが保証されること。  
回帰モデルの多項式の次数を増やすと、次数の増加に伴い、係数の値が増大し、訓練データに適合しすぎてしまう。  
最小二乗でモデルのパラメータを求めるのは、最尤推定での特別な場合に相当する。  
→過学習が最尤推定の一般的な性質である。  
過学習の問題を避けるために、ベイズ的アプローチが有効。  
正則化の概念について説明。  
二次の正則化＝Ridge回帰＝ニューラルネットワークの文脈で荷重減衰  

### 1-2
確率論の基礎的な概念の解説。  
確率の同時分布の対称性と、情報定理からベイズの定理が導かれる。  
事前確率：観測するより前にわかっている確率  
事後確率：一度事象が確認されてからベイズの定理から求められる、推定した事象の確率  
今まで直感的に使ってきた確率はランダムな繰り返しの頻度とみなされる、古典的確率、頻度主義的な確率解釈。
ベイズの定理を機械学習に当てはめるとすると  
p（w|X） = p(X|w)p(w) / p(X)  
パラメータの事後確率　＝　モデル・パラメータの事前分布 / 規格化定数  
事象xを確認した後に、wに関する不確実性を事後分布の形で評価する。  
不確実性を定量的に表現し、新たなデータで修正していく方法をベイズ的な確率解釈は実現可能。  
p(X|w)を尤度といい,尤度を最大化させる方法を最尤推定法という。  
ベイズの定理は  
事後確率 ∝ 尤度 × 事前確率  
最尤推定の気持ちとしては、データを生成する確率を最大にするパラメータがいいパラメータだろうという考え。  
最尤推定の問題点  
・尤度p(X|w)は厳密には確率ではない  
・単純に尤度を最大化すると過学習しやすい  
・モデル選択が難しい場合が多い。   
ベイズ的なアプローチの利点は、事前知識を、事前確率として自然にモデルに入れることができる点。  
公平に見えるコインを３回投げて毎回表がでたとしても、表がでる確率を頻度主義的な考えだと１になるのを防ぐことができる。  
マルコフ連鎖モンテカルロ法MCMC法のようなサンプリング法の開発がベイズの定理を実用化させてきた。  

#### 1-2-4
ガウス分布の性質についての内容。  
ガウス分布は、平均と分散によって定められ、全領域で積分すると１になるため確率分布の用件を満たす。  
ガウス分布での最頻値と平均は一致する。  
同一の正規分布から独立にデータがN個生成された場合のデータ集合の確率（尤度）は、正規分布の積で表される。  
最尤推定で求めたパラメータの期待値はE=N-1/N σ^2で、バイアスのある推定量  
データ点の数が増えればバイアスの影響はなくなる。データ量が多い理由の一つ。  

#### 1-2-5
最尤法を用いて曲線フィッティングを行う。  
ノイズが正規分布から発生していると考えると、モデルは正規分布の形で書くことができる。p(t|x,w,β) = N(t|y(x,w), β^-1)  
ここから全てのデータからの尤度を求めて、最尤推定法によりパラメータを求める。  
データに基づいた事後確率を最大にするパラメータ推定法を最大事後確率（MAP）推定という。  

#### 1-2-6
完全なベイズアプローチではwのすべての値に関して積分する必要がある。  
予測分布はガウス分布の形で与えられる。  

### 1-3
モデルの性能評価をまとめた内容  
交差分割検証:訓練データをs分割、訓練時間はs倍。  
情報量規準：赤池情報量規準AIC、ベイズ情報量規準BICを用いる。  

### 1-4
データをます目に分割すると、入力変数が増加するとマス目の数が指数関数的に増大する。  
多項式曲線フィッティングを複数個の入力変数に拡張した場合、より高次の多項式が必要となり、係数の数はべき乗に増える。  
N次元の球の体積は表面に近い薄皮部分に集中するという、幾何的直感と一致しない  
大きい異次元空有間に伴う困難全般のことを次元の呪いとよぶ  
実用では目的変数の変化を生じさせる方向は限られているため、意外と高次元でもなんとかなる。

### 1-5
クラス分類や、回帰予測の値の決定は誤識別率が最小になるように決定される。  
最大事後確率を
最大事後確率をもつクラスに決定される。  
識別率だけ上げると、問題になる分類もある。→損失関数を定めて、期待損失を最小化させる。  
決定問題を解く時の三つのアプローチ  

・生成モデル：　　
クラス事後確率p(C|x)、同時分布を推論する。出力の分布だけではなく、入力の分布もモデル化→モデルからのサンプリングで入力空間で人工データを生成できる。  
データの周辺分布p(x)を決めることができるため、低い確率をとる新しいデータ点をみつけ、外れ値検出、新規性検出に用いることができる。  
同時分布p(x,C)を推定するのにデータを大量に消費するため、事後確率p(C|x)のみ必要な場合は識別モデルを用いれば良い。  

・識別モデル：  
事後確率のみを推論して決定論でクラス割り当て。出力の分布をモデル化  

・識別関数：  
入力から直接クラスラベルに写像する関数f(x)をみつける。確率は用いいない。   

事後確率を求める利点：
訓練データの修正による事前確率の修正が容易、棄却オプションを変更可能、複数のモデルの結合が容易  

回帰の場合は、損失関数を二乗誤差とする。  
最適解は、xが与えられた下でのtの条件つき平均で、回帰関数といわれる。  

損失関数＝MSE＋目標データが持つノイズ  
と表され、損失関数の最初値はデータのノイズで決まる。  

### 1-6
離散確率変数xを観測した時の情報量 h(x) = -log{p(x)}
確率が低い事象が起こった時ほど大きな値をとる。
h(x)の単位はビット
情報量の平均をエントロピーと呼ぶ.

#### 1-6-1
真の分布p(x)をq(x)で近似する。q(x)でxの値を特定するのに追加で必要な情報量はKLダイバージェンスで求められる。  
KLダイバージェンス＝（p(x)のエントロピー）　ー　（q（x）のエントロピー)  
KL大バージェンンスの最小化＝尤度最大化  

## ２章
詳しい解説スライド  
https://www.slideshare.net/TakutoKimura/prml21-222425  
まとめスライド  
https://www.slideshare.net/matsuolab/prml2

演習問題の解答  
https://drive.google.com/drive/folders/0Bz9yuvZCp4qSY05fWDVqOHpzSmc  

有名な確率分布やその特徴をまとめた章。  
ベイズ推論をについても触れる。  
パラメトリック：  
ガウス分布の平均、分散など、少数のパラメータにより確率分布が決定される  
パラメタの値を決める手段は必要になり、そのうちの一つが尤度関数の最適化となる。  
ベイズ主義の立場では、パラメータに事前分布を導入して、観測データが与えられた場合のパラメータの事後分布をベイズの定理を用いて計算する。  

ノンパラメトリック：少数のパラメタでは確率分布が決まらない。一応パラメタはあるが、モデルの複雑さを決めるために使われる  

### 2-1
ベルヌーイ分布：コイン投げによる確率分布  
０がでる確率μのみでモデルが決定される。μについて尤度関数を微分して解くと、μの最尤推定量が求まる。  
＝サンプル平均sum(x)/N たまたま３回表が出た時に、最尤推定では必ず表がでるという推定結果になる。  
二項分布：コイン投げにおける表のでる回数  

#### 2-1-1
コイン投げの例のように最尤推定では、サンプル数が少ないと過学習を起こしやすいため、事前分布を考えるベイズ主義的に扱う。  
妥当なモデルを事前分布に採用←妥当性は検証する必要あり  
ベルヌーイ分布のパラメタμの事前分布にベータ分布を用いる。←共役性のため  
逐次学習：  
新たなサンプルが追加されることで、尤度関数を更新し、事後分布が更新される方法  

### 2-2
ベルヌーイ分布、二項分布の他変数への拡張を行なっている。＝多項分布  
他変量の場合の対数尤度の最大化はラグランジュの未定乗数法を用いて行える。
多項分布のパラメタμkの事前分布としてディリクレ分布を採用  

### 2-3
ガウス分布についての内容。  
複数の確率変数の和の確率分布はガウス分布になる(中心極限定理)  
確率変数が互いに独立であるこが必要  
ガウス分布はマハラノビス距離を通して、xに依存する。  
ガウス分布は単峰形（極大値が一つ）という条件から、多峰形の分布をうまく近似できない。  
→潜在変数を導入することで解決。  

#### 2-3-1
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、条件付き分布p(xa|xb) もガウス分布に従う  
→どのように同時分布の断面を切ってもガウス分布
この証明を説明している。  
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、周辺分布p(xa), p(xb)もガウス分布に従う。  
やっている内容は、ベクトルxを二つの部分ベクトルx = (xa,xb)に分割したガウス分布p(x）を考え、条件付き分布p(xa|xb)と周辺分布p(xa)を求めている。  

#### 2-3-3
ガウス分布における変数で、ベイズの定理的理解をする節
ガウス周辺分布p(x)と平均がxの線型関数で共分散はxとは独立であるガウス条件付き分布p(y|x)を定義している。  
＝線型ガウスモデル  

#### 2-3-4
多変量ガウス分布から標本Xが得られた時、母集団の多変量ガウス分布のパラメタを最尤推定で推定できる。  
対数尤度関数は、ガウス分布の十分統計量にのみ依存している。  
最尤推定による平均は、データXの平均と一致し、共分散も通常の共分散の平均を最尤推定に置き換えたものになる。  

#### 2-3-5
ガウス分布の最尤推定において、逐次推定を行う。  
誤差信号の方向へ推定量を更新。  
Robbins-Monroアルゴリズム＝一般的な最尤推定問題を逐次的に解く方法。  

#### 2-3-6
ガウス分布における最尤推定の枠組みで、パラメータ上の事前分布を導入してベイズ主義的な扱いをする。  
観測データが生じる確率である尤度関数は、平均μの関数とみなせる。  
事前分布p(μ）にガウス分布を選べば、尤度関数の共役事前分布となる。  
平均を既知として、分散を推定する場合は共役事前分布ガンマ分布を用いる。  
平均も分散も未知として推定する場合は、事前分布に正規-ガンマ分布を考える  

#### 2-3-7
スチューデントのt分布についての説明。  
μ＝１でコーシー分布となる。  
平均は同じで、精度の異なるようなガウス分布を無限個足し合わせたもの。  
ガウス分布に比べ裾が広く、外れ値に対してロバストである。  
t分布に対する最尤推定会はEMアルゴリズムによって求まる、  

#### 2-3-8
ガウス分布に周期変数を導入する。  
周期変数の観測値の集合は曲座標表示可能。  
フォンミーゼス分布：確率変数、統計量を極座標表示することで、ガウス分布の周期変数への一般化を行なっている分布、  

#### 2-3-9
混合ガウス分布：ガウス分布を線形結合することでタータの分布を表現する  
線型結合する重みの係数と各分布の平均と共分散を調節すれば、任意の連続な密度関数を任意の精度で近似できる。  
混合係数は確率の条件を満たしており、k番目の混合要素を選択する事前確率と捉えられる。
事後確率p(k|x)＝負担率とする  
最尤推定は解析解が難しい→EMアルゴリズムの出番  

### 2-4
指数型分布族の全般的な性質について説明した内容  
→ベルヌーイ分布、多項分布、ガウス分布  
ベルヌーイ分布の確率分布からシグモイド関数を導ける。  
多項分布は二項分布の一般化版。  
最尤推定の解は十分推定量のみによって決まる。  

#### 2-4-2
共役事前分布についての一般的な説明。  
ある確率分布p(x|λ)について、事後分布がその事前分布と同じになる関数系になる、尤度関数と共役な事前分布p(λ)を求めることは可能。  

#### 2-4-3
無情報事前分布：事前分布がわからない時に、できるだけ事後分布に影響を及ぼさないような分布。  
並行移動不変性、尺度不変性がある。  

#### 2-4-5
パラメトリック法：  
少数のパラメタから確率変数の分布の形状を決める。  
あらかじめ確率分布の形状を仮定する。  
仮定した分布が不適切だと、予測性能が悪くなる。  

この節ではノンパラメトリック方について扱っている。  
分布の形状に制限されず、データによって形状を決定する。  
・ヒストグラム密度推定法  
連続値をヒストグラム化させ、データ集合を元にした分布を作成する。  
区間の幅が大きすぎるとモデルがなだらかになりすぎて、うまく分布の特性を捉えられない。  
いったん推定すれば、データを破棄できるため大規模データに有効  
・カーネル密度推定法  
観測点xを含む微小領域Rを考え、R内の点の総数Kが二項分布に従うことを用いる。  

・最近傍法  
カーネル密度推定ではカーネル幅を決めるパラメータhがすべてのカーネルで一定という問題を解決した手法。  

## 3章
線形回帰モデルを解説した章  
入力xに対し的固定された基底関数の線形結合をとる。  
パラメータに対しては線型で、入力変数に対しては非線形になりうる。  
確率論的には：  
xにたいするtの値の不確かさを表すために予測分布p(t|x)をモデル化  
損失関数の期待値を最小化するように任意のxの新しい値に対するtを予測可能。   

解説スライド  
3-2まで
https://www.slideshare.net/sotetsukoyamada/prml3332  

https://www.slideshare.net/matsuolab/prml3-78265931  
3−４まで  
https://www.slideshare.net/ryosukeinform/prml-3  
3-5以降  
https://www.slideshare.net/yukimatsubara9847/prml35-40730590

### 3-1
線型基底関数モデル：最も単純な線形回帰モデル。入力変数の線形結合y=sum(wx)  
パラメータwに対して線型かつ入力xに対しても線型  
→表現力が乏しいので、非線形な基底関数の線形結合を考える。y=sum(wφ(x))  
パラメータwに関しては線型なので、線型モデルと呼ばれる。  
すでに考えた多項式回帰
多項式回帰では、入力空間の一部の変化が他の領域に影響を及ぼしてしまう問題を、入力空間を分割し、各領域で多項式を当てはめる＝スプライン関数  
基底関数の例に、多項式基底、ガウス基底、シグモイド基底があげられる。  
tanh関数はシグモイド関数で表されるので、シグモイド関数の線形結合とtanhの線形結合は等価。  
この章では、基底関数のとりかたに依存しないので、特に指定していない。  

#### 3-1-1
最小二乗法と最尤推定の関係を説明する節  
目的変数tを関数がガウスノイズの和で与えられる場合を考える。  
t = y(x,w)+ ε  
tはガウス分布で表される状況になる。p(t|x,w,β)=N(t|y(x,w),β^-1)  
この場合の尤度を考える。  
線型ガウスモデルの条件では、尤度最大化と二乗誤差最小化は等価  
対数尤度を最大化させると最小二乗法での正規方程式が得られる。  
バイアスw0は目標値の平均と基底関数の値の平均の重み付き和との差を埋め合わせる役割を持つ。  

#### 3-1-2
最小二乗法の幾何学的な解釈  
データ点からモデルの貼る空間への射影が最尤推定により定まる。  
2つ以上の基底ベクトルが線型従属の時、

#### 3-1-3
逐次的学習のおさらい。確率的勾配効果法SGDを説明。

#### 3-1-4
正則化項を加えた最小二乗法の説明  
正則化項は機械学習の分野では荷重減衰、統計学の分野ではパラメータ縮小推定と呼ばれる。    
lassoがスパースになりやすい理由を図的に説明  

#### 3-1-5
目標ベクトルtの様子に対して異なる基底関数を用いれば、多次元出力の問題は、一次元回帰問題に帰着される。  
実際に使われているアプローチは、同じ基底関数を目標ベクトルの全ての要素に対してモデル化するもの  

### 3-2
最尤推定、最小二乗法の問題：  
過学習をしやすいので、基底関数の数を減らす→モデルの表現力が減る  
正則化項の導入→正則化係数λの決定が難しい。  
ベイズ的にパラメータを周辺化すれば解決する。  
この節はバイアス、バリアンスのトレードオフと知られるモデルの複雑さを説明  

予測yの評価をしたい期待二乗損失はbias^2 + variance + noiseに分解できる。  
もし、無数のデータが利用可能なら、原理的には任意の精度の回帰関数を求められるが、現実には有限データのみしか扱えないので不可能  
学習アルゴリズムの性能はデータ集合のとり方に関して平均することで評価する。  
真の分布と、回帰関数の期待二乗誤差は、バイアスの二乗とバリアンスの和で表される。  
バイアス：全てのデータ集合のとりかたに関する予測値の平均が理想的な回帰関数からどれくらい離れているかを表す。  
バリアンス：データが変わると、予測値がどれだけ変わるかを表す。  
学習の目標は、二乗バイアス、バリアンス、ノイズだけに依存する期待損失を最小化すること。  
ノイズは、真の予測分布がわからないと行けないので、求めることができない。  
バイアス大、バリアンス小：柔軟性のある複雑なモデル  
バイアス小、バリアンス大：柔軟性の低い  

### 3-3
最尤推定：  
モデルの複雑さをデータに依存して決める。正則化項でモデルの複雑さを調整。過学習の可能性が高い。訓練データの一部をテストデータにするのがもったいない。  
ベイズ線形回帰：  
パラメータを確率変数として扱う。過学習を回避できる。訓練データだけで、モデルの複雑さを推定可能  

#### 3-3-1
線形回帰モデルの尤度関数はwの二次関数の指数なので、モデルパラメータwの事前分布をガウス分布として考える。(ベイズ的発想)  
事後分布は尤度関数×事前分布なので、ガウス分布となる。  
事後確率はガウス分布なので、最頻値は期待値と一致し、事後確率を最大にする重みベクトルは単純に期待値となる。  
議論を簡単にするために、期待値０で単一精度パラメータαで表される等方的ガウス分布を考える。＝wの事前分布  
事後分布の対数をwに関して最大化するには、二乗誤差関数と二次正則化項の和を最小にすることと等価。  
y=w0+w1xの1次元の線型モデルを考える。  
データからパラメータを復元し、データのサイズと推定値との関係を明らかにする。  
逐次更新：初期値を適当に取り出して尤度関数を求める。尤度関数と事前分布をかけて事後確率を求める。事後確率から適当にパラメータを決め、関数を推定。以上を繰り返す。 

#### 3-3-2
実際の場面ではwの値そのものより、新しいxに対するtを予測したい。  
予測分布の評価をする必要がある。  
予測分布の分散は、新しいデータが追加されると事後分布は狭くなっていく。  
予測分散＝データに含まれるノイズ＋wに関する不確かさ  

#### 3-3-3
線型基底関数モデルに対して、事後分布の平均解を求めると、点xでの予測分布の平均は訓練データの目標変数tの線型結合になる。  
y=sum(k(x,x)t)  
k(x,x)を等価カーネルと呼び、訓練データの目標値の線型結合をとることで予測を行う回帰関数を線型平滑器とよぶ   
基底関数の集合を明示的に定義するのではなく、局所的なカーネルを定義し、新たな入力ベクトルxに対する予測値を求められる＝ガウス過程  
等価カーネルは重みを定める役割をはたす。  

#### 3-4
ベイズモデルの比較に関する内容  
ベイズモデルでは最尤推定で問題になった過学習を点推定する代わりに周辺かすることにより回避  
訓練データのみで評価可能なため、交差確認を回避できる。  
モデルの複雑さを決めるパラメータを複数導入する。  
データ集合D情のモデル集合{Mi}からモデル選択をベイズ的に行う  
p(M|D)∝p(M)p(D|M)  
モデルエビデンスp(D|M)というモデルでデータがどれくらい説明できるかという項を定義=周辺尤度  
モデルの事後分布がわかれば、予測分布はここのモデルの予測分布の事後分布の重み付き平均で得られる。  
パラメータが一つの場合と複数の場合でのモデルエビデンスの解釈を行なっている。  
でーたの分布p(D)の対数は、データへのフィッティング度とペナルティ項の和で表される。  

### 3-5
線型基底関数モデルを完全にベイズ的に取り扱うために、ハイパーパラメータα、βに対しても事前分布を導入し、通常のパラメータwだけでなく、ハイパーパラメータに対しても周辺化し、予測を行う。  
予測分布はw,α、βに関して周辺化することで得られる。  
事前分布p(α、β|t)が特定の周りで鋭く尖っている場合、α、βをそれぞれの値に固定して単にwに周辺化することで予測分布が得られる。  

#### 3-5-1
周辺尤度関数p(t|α、β）＝エビデンス関数は同時分布をwに関する積分で得られる  
ヘッセ行列などをもちいて、周辺尤度の対数＝対数エビデンス関数を求める。  

#### 3-5-2
エビデンス関数p(t|α、β）を最大化する問題を考える。  
前の節で求めた対数エビデンス関数をαについて微分した式からγを定義して、α、βについて停留点を求めることで得る  

#### 3-5-3
前節のγは有効なパラメータ数に対応。  
ある統計量を構成する変数のうち、独立な物の数をその統計量の自由度という。  

#### 3-5-4
非線型基底関数を線形結合したモデルを考えてきた。  
利点：
最小二乗問題の閉じた解がもとまる。  
ベイズ推定の計算が楽になる。  
規定関数をうまく選べば、任意の非線形変換モデルを作成できる。  
欠点：  
データの観測前に基底関数を固定するため、入力空間の次元数に応じて指数的に基底関数の数を増やす必要がある（次元ん呪い）  

## 4章
4.4-4.5解説  
https://www.slideshare.net/yukimatsubara9847?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview  
松尾研まとめ  
https://www.slideshare.net/matsuolab/prml4-78265978  

入力xに対して、離散クラスCkを割り当てる＝入力空間をいくつかの決定領域に分離する。  
決定領域の境界を決定境界、決定面と呼び、D次元入力空間に対して、D-1次元の超平面で定義される決定面のことを線型識別モデルと呼ぶ。   
線形決定面で正しくクラス分類できるデータ集合を線形分離可能であるという。  
分類問題に対するアプローチは３つに分けられる  
1,識別関数を構築する方法・・・パーセプトロン、SVM  
２、事後確率＝条件付き確率分布p(C|x) を直接モデル化する方法（識別モデル）  
３、p(C)とp(x|C)を生成し、これらからp(C|x)を求める方・・・ナイーブベイズなど  
これまでの線形回帰モデルは、実数値yを求めていたが、分類問題では離散値をとりたいため、非線形関数である活性化関数へ入力する。  
活性化関数は非線形であっても、決定面は線形であるから、y=f(wx+b）で表されるモデのクラスを一般化線形モデルと呼ぶ  

### 4-1
この章では決定面が超平面y(x)=0になる線形識別のみを扱う。  
n次元空間における超平面とは次元がn-1の部分空間を表し、1つの超平面は全体空間を２つの半空間に分割する。  

#### 4-1-1
最も簡単な線形識別関数は入力ベクトルxの線形関数y(x)=wx+w0で与えられる。  
w0はバイアスパラメータとよばれ、-w０はしきい値パラメータと呼ばれることもある。  
y(x)>=0なら入力ベクトルはクラスC１に分けられ、それ以外はクラスC2に分けられる。  
対応する決定境界は　y（x）＝０で定義される。  

#### 4-1-2
K=２の線形識別をK>2に拡張する。  
１対他分類器：ある特定のクラスCkに入る点と、それ以外のクラスに分類する２クラス問題を解く分類器をK−１個利用する方法。分類が曖昧な領域が生じてしまう。  
１対１分類器：すべての可能なクラスの組の２クラス識別関数を考え、K(K-1)/2個の２クラス分類関数を導入する方法。いずれも曖昧な領域が存在する。  
曖昧な領域をなくす方法  
yk(x)=wkx+wk0のK個の線形関数を作り、yk(x)>yj(x)が成り立つ場合にxはCkに分類されるとする  

#### 4-1-3
3章で行なったようにパラメータに関する線形モデルを考え、二乗和誤差の最小化によってパラメータを解析に求めた手順を分類問題に応用する  
各クラスCkは各クラスごとの線形モデルで記述される。yk=wkx+w0  
ベクトル表記を用いて記述するとy=Wxとなる   
二乗話誤差を最小化してパラメータ行列Wを決定する。  
目的変数のベクトルをTとすると、二乗話誤差関数はE=1/2Tr{(XW-T)(XW-T)}となる。  
Wに関する導関数を０として、整理すると、識別関数がもとまる。  
最小二乗解の問題点：  
外れ値に敏感（頑健でない）,最小二乗法は条件付き確率分布にガウス分布を仮定した場合の最尤法であり、2値目的変数ベクトルは明らかにガウス分布からかけ離れているので、最小二乗法がうまく使えない。  
→適切な確率モデルを採用すれば、最小二乗法よりも良い特性を持つ分類法が得られる  

#### 4-1-4
これまで考えてきた線形識別モデルはD次元の入力ベクトルを1次元空間に写すので情報の損失が発生する。  
→D次元空間では分離されていたクラスが1次元空間では重なり合う可能性がある  
・D次元の入力モデルを１次元に射影している。  
重みベクトルwを調整することで、うまく分類できる。=クラスの平均を結ぶ直線の正射影の長さが最大になるwをとる。  
y=wxは入力空間x内のラベル付けされたデータ集合を1次限空間y内のラベル付けされたデータ集合へ射影する。  
クラスCkから射影されたデータのクラス内分散を求めて、クラス内分散とクラス間分散の比をフィッシャーの判別基準呼ぶ  
フィッシャーの判別基準はクラス間分散行列とクラス内共分散行列を使って書くことができる。  
データの次元を１次元へ削減する際のデータの射影方向の選択を行える方法を、フィッシャーの線形判別と呼ぶ  

#### 4-1-5
#### 4-1-6
フィッシャーの判別基準が最小二乗の特殊な場合になっていることを示す。  
クラスC１に対する目的変数値をN/N1ととり、クラスC２を-N/N2ととれば、重みに対する最小二乗解がフィッシャの解と等価になる。  

#### 4-1-7
線形識別モデルとしてパーセプトロンを取り上げている。  
2クラスのモデルで、入力ベクトルxを特徴ベクトルに変換する非線形関数φを用いて  
y(x)=f(wφ(x))  
という形の一般化線形モデルを用いる。非線形活性化関数fはステップ関数をとる。  
活性化関数の相性を考え、目的変数の表記はt={-1,1}  
wの決定アルゴリズム （誤差関数の選択）  
誤差関数には、ご識別したパターンの総数とするのが自然。  
決定境界がデータ点を横切る度に不連続となり誤差関数の勾配が０となってしまう。  
パーセプトロン基準という誤差関数を用いるE=-sum(wφT)  
誤差関数の最小化に確率的最急降下アルゴリズムを用いる  

パーセプトロンの問題点:  
パラメタの初期値やデータの提示順に依存して様々な解に収束してしまう。  
収束に必要な回数が多いため、収束可能かどうかわかりにくい。  
線形分離不可能な場合は収束しない  
確率的な出力をしない  
K>2クラスの場合への一般化が難しい  


### 4-2
生成的アプローチ：尤度、条件付き確率密度p(x|Ck)と、事前確率p(Ck)からベイズの定理を用いて事後確率p(Ck|x)を計算する  
クラスC１に対する事後確率をロジスティックシグモイド関数を使って書き直すことができる。  
Kクラスの場合の事後確率p(Ck|x)は、ソフトマックス関数で書くことができる。  

#### 4-2-1
クラスの条件付き確率密度がガウス分布であり、全てのクラスが同じ共分散行列を共有すると仮定  
この時の尤度p(x|Ck)は共分散行列で表される  
共分散行列が共有されている場合、決定境界は入力xの線形関数になっている  
共分散行列が共有されていない場合は、決定境界はxの2次関数となる。  

#### 4-2-2
尤度p(x|Ck)と事前確率p(Ck)をパラメトリックな関数型で表現しておき、パラメータを最尤法でもとめる  
尤度関数p(t,X|π,μ１、μ２、Σ)を定める。  
クラスの事前確率p(C1)=π、p(C2)=1-πでのπに関する最尤推定はC1のデータの個数の割合になる。  
ガウス分布の最尤推定が外れ値に対して頑健でないので、クラス分布にガウス分布をフィット場合は注意。  

#### 4-2-4
クラスの条件付き確率密度p(x|Ck)が指数型分布族のメンバーの時の一般的な結果を考える。  
K=2,K>2クラスの時も、決定境界はxの線形関数になる。  

### 4-3
今までの例：  
クラスの条件付き確率密度のパラメータとクラスの事前確率p(Ck)を最尤法により決定でき、ベイズの定理を用いてクラスの事後確率を求めることができた。  
生成的アプローチといえ、周辺分布p(x)から人工的にデータxを生成できる
これから：  
一般化線形モデルの関数形式を明らかに仮定し、最尤法を利用してパラメータを決定する方法を扱う。  
＝反復再重み付け最小二乗法IRLS  
識別アプローチ  

#### 4-3-1
入力空間が線形分離不可能であっても、非線形変換φをうまくとれば、特徴空間上で線形分離可能となる。  
非線形変換を行なってもクラス間の重なりは取り除くことができない。  

#### 4-3-2
ロジスティック回帰についての節。回帰とあるが、分類のためのモデルに注意(おきまり)  
p(C1|φ)=y(φ)=σ(wφ)  
ロジスティック関数のパラメータ数は特徴空間の次元Mに対して線形依存する。  
ガウス分布のクラス付き条件確率密度を最尤法を使う場合はパラメータ数はO(M^2)になる  
他クラスにおける尤度関数はp(t|q)=Πy^t(1-y)^1-t  
誤差関数に尤度の負の対数をとった、交差エントロピー誤差関数を用いる。  
wに対する誤差関数の勾配は▽E=sum(y-t)φ  
データnの勾配の寄与は、目的地と予測値の誤差y-tと基底関数ベクトルφの積で与えられる  
▽E=0の解をニュートン法で解くと、

#### 4-3-3
３章での線形回帰モデルは最尤解を解析的に導出できる。←対数尤度関数がwの二次関数となるため。  
ニュートンラフソン法による、尤度関数の局所二次近似を利用して、反復最適化手段を用いた最小化方法  
ヘッセ行列H：Eの二階微分  
ニュートンラフソン法の式を解くために、二乗和誤差関数の微分と、ヘッセ行列を求める。  

#### 4-3-4
他クラスへのロジスティック回帰の拡張を行う  
事後確率はソフトマックス関数で与えられ、尤度関数は各データの事後確率の積和になる。  
目標：最尤法によりwを求める。  

#### 4-3-5
今まではクラスの条件付き確率密度が指数型分布族のときを見てきたが、混合ガウス分布の時はうまく計算できない。  
ここでは別のタイプの識別確率モデルを考える。  
雑音しきい値モデルを考えている。  
出力aが閾値θより大きい場合にクラス１とする。θは確率的な項で確率密度関数gにしたがっている。  
特にgが平均０、分散１の標準ガウス分布に従う時、このモデルをプロビット回帰と呼ぶ  
ロジスティック回帰と似た結果になるが、外れ値に対してはより敏感になる。  

#### 4-3-6
▽E=sum(y-t)φを目的変数tの分布が指数型分布族のときに一般化した式を得るのが目的  
fを非線形関数としてy=f(wφ)というモデルを考えるとf^-1は連結関数と呼ばれる  


### 4-4
ロジスティック回帰のベイズ的な取り扱いのために必要なラプラス近似を説明  
ラプラス近似の目的：  
分布p(z)が与えられた時に、モードz0を中心とするガウス分布p(z)を近似すること  
1変数の場合を考えて、分布p(z)を仮定p(z)=1/z f(z)   
p(z)モードを求める＝p(z0)=0をまたは、df(z)/dz =0を満たすz0を見つける  
ガウス分布の対数は二次関数→log(z)をテイラー展開して、zの２次の項までまとめる  
z0が分布の局所最大値なので、テイラー展開の1事項は現れない  
テイラー展開の指数をとりf(z）を決定し、正規分布q(z)を得る  

#### 4-4-1
分布p(z)を近似するのと同様に、正規化係数Zの近似を得ることができる。  
データ集合Dとパラメータ{θ}をもつモデルの集合{M}を考える  
ベイズの定理からモデルエビデンスp(D)が求められる。  

### 4-5
ロジスティック回帰をベイズ的に取り扱う。厳密に行うことは困難なので、ラプラス近似を使って計算を行う。  

#### 4-5-1
ラプラス近似には事後分布の対数の２階微分の評価が必要＝ヘッセ行列をみつける  

事後分布がガウス分布なら、事前分布もガウス分布とするのが自然。  
p(w)=N(w|m0,S0)  
として、wの事後確率分布は  
p(w|t) = p(w)p(t|w)  
事後確率＝尤度の対数を簡単に表記  
事後確率を最大化して、MAP（最大事後確率）解Wmap を求める。  
Wmapはガウス分布の平均を定義する。  
共分散行列をSNを求めて、事後確率分布のガウス分布の近似を得る  
近似されたガウス分布を周辺化することで予測ができる。  

#### 4-5-2
クラスC1の事後確率の予測分布p(C1|φ,t)を求める。  
最終的に、ロジスティックシグモイド関数での畳み込み積分をあらわしているので、解析的に評価できない  
ロジスティックシグモイド関数とプロビット関数の逆関数の類似性を利用すれば、いい近似を得ることができる。  

## 5章
5.4~5.4.6の解説  
https://speakerdeck.com/hassaku/prml-chapter5-hessian-matrix?slide=12  
5.5.6~5.5.7  
https://www.slideshare.net/RicksonJr/bbb-15673191  
名古屋大の輪講スライド  
https://www.slideshare.net/t_koshikawa/prml-5-pp227pp247  
松尾研のスライド  
https://www.slideshare.net/matsuolab/prml5-78266012  

これまでは線形回帰を行ない、解析的に解ける利点がある一方、次元の呪いの問題がある。  
解決する手法  
SVM:訓練データ点を中心とした基底関数群を定義し、訓練中に基底関数を選択する。  
ニューラルネットワーク：基底関数の数を固定し、適応的にしていく。  
多層パーセプトロンは、パーセプトロンを重ねたものよりは、ロジスティック回帰を重ねた物になる。  

### 5-3
これまでの回帰、分類線形モデル  
y(x,w)=f(sum(wφ))  
φは非線形基底関数、重みの線形結合、  
活性化関数f:分類→非線形、回帰→恒等  
章の目標：基底関数φ(x)をパラメータ依存するように拡張、重みパラメータwと基底パラメータを同時に学習,非線形ロジスティック回帰へと拡張  
a=sum(wz)+w0を活性とよび、活性を微分可能な非線形活性化関数hで変換され  
z=h(a)となる。  
基底関数は、ニューラルネットワークでは隠れユニットと呼ばれる  
行列演算と、非線形変換を繰り返す。  
ニューラルネットワークは、調整可能なパラメータ集合wによる、入力Xから出力Yへの非線形関数  
NNはパーセプトロンと似ている感じがあるが、隠れユニットで連続な非線形性を持つシグモイド関数を持ち、非線形性をもつステップ関数をもつパーセプトロンとは決定的に異なる。  
NNによる関数はネットワークパラメータにおいて微分可能  
活性化関数を線形にすると。隠れ素子を持たない等価なNNが必ず存在。  
連続的な線形変換はそれ自体が線形変換となる。  

NNはフィードフォワードであることが必要＝閉じた有効閉路がない。  
NNは万能近似器：線形出力を持つ２層ネットワークは、十分な数の隠れユニットがあればどんな連続関数も任意の精度で近似できる。  
同じ入力から出力への関数を表す重みベクトルwは複数存在  
活性化関数の対称性、線形における交代性から、隠れ層のユニットがM個あった場合、ネットワークにはM!2^M個の等価な重みベクトルがある。  

### 5-2
NNの訓練過程について説明してる内容  
NNを入力Xから出力Yへのマッピング関数と捉えれば、今までの議論が適応できる。  
回帰の時と同様に二乗話誤差の最小化を考える  
ネットワークの出力を確率的に解釈すると、より一般的な学習を考えることが可能  
目的変数tがガウス分布に従うと仮定  
p(t|x,w)=N(t|y(x,w),β^-1)  
尤度関数はp(t|X,w,β)=Πp(t|x,w,β)  
この負の対数をとると、誤差関数が得られる。  
重みパラメータwについて最適化するために、βを固定すると  
E(w)=1/2sum(y(x,w)-t)^2となり  
尤度最大化＝二乗和誤差最小化となる  
二値分類の場合の、条件付き確率分布はベルヌーイ分布になる  
負の対数尤度で与えられる誤差関数を交差エントロピー誤差という。 
クラス分類問題では、二乗和の代わりに交差エントロピー誤差関数を使う方が訓練が高速になり、汎化性能が高まる。  

#### 5-2-1
目標：E(w)が最小値をとるようなベクトルwを見つける  
極小点を探索するアルゴリズム:  
重みの初期値w0を定義  
連続的なステップ移動 w(t+1)=w(t)+Δw(t)  

#### 5-2-3
極小点を探索するのに、勾配情報を用いない場合はO(N^3)の計算量が必要になり、逆誤差伝搬を行うとO(N)の計算量に減らすことができる。  


#### 5-2-4
最もシンプル方法：勾配降下法  
w(t+1)=w(t)-η▽E(w(t))  
全てのデータ集合を一度に使うテクニック：バッチ学習  
直感的に合理的なではあるが、実際には性能が悪いアルゴリズムになっている。  
勾配降下法の特徴:  
収束が非常に遅い。収束性が初期値の選び方に依存しない。学習率の設定が面倒くさい。局所最適化解にはまる。  
バッチ最適化にはニュートン法、共役分配法がある。  
ニュートン法の特徴：  
二回微分の情報も用いて更新。最急降下方より早い収束が可能。ヘッセ行列の逆行列が不安定になる危険性がある。  
バッチ学習：全ての学習データで誤差関数を評価。E=sum(En)  
オンライン学習：1個の学習データで誤差関数を評価E=En  
ミニバッチ学習：複数の学習データで誤差関数を評価E=sum(En~m)  

#### 5-3
誤差逆伝搬についての説明内容  
2つのステップからなる。  
1:誤差関数の重みに関する微分を評価  
2： 勾配を用いたパラメータの最適化クリック。  
誤差関数は二乗和だけでなく、交差エントロピー、ヘッセ行列、ヤコビ行列などにも対応。  

#### 5-3-1
重みの勾配＝接続先素子の勾配✖️接続先素子の出力  
誤差逆伝搬法の具体的な計算過程をしめしている。  
この節の内容はオンライン学習で、バッチ学習の場合は一つ一つの誤差関数に関して計算された勾配の和をとる  
ミニバッチ学習の場合は、ミニバッチ数分だけ勾配の和をとればよい  

##### 5-3-2
誤差逆伝搬法の例として、二乗和の誤差関数をもち、出力ユニットは恒等関数、２層ネットワークで、各r層の活性化間数はtanhを考える.  

#### 5-3-3
誤差逆伝搬法の計算効率は、ネットワークの重みとバイアスの総数をWとするとO(W)になる。  

#### 5-3-4
ヤコビ行列への誤差逆伝搬法の適応を行なっている。  

#### 54
これまでは、誤差関数のネットワーク重みに関する１階微分が、逆伝搬でどのように得られるかをみたが、  
逆伝搬は２階微分を評価するのに用いることができる。  
誤差関数の重みの二回微分は、ヘッセ行列Hの成分をなす。  
ヘッセ行列はベイズニューラルネットワークのラプラス近似において中心的な役割をはたす。  
評価するために必要な計算量はO(W^2)となる。  
ヘッセ行列の計算を減らす方法がいくつか提案される  

#### 5-4-1
多くの場合は、ヘッセ行列自身より、逆行列が必要になる。よって、ヘッセ行列を対角行列で近似するとよい。  
対角行列の逆行列は、成分の逆数をとったものになるため、簡単に逆行列を求めることができる。  

#### 5-4-2
誤差関数が平方和になる場合は、十分に訓練された状態では、ヘッセ行列の一部の項は無視できるくらいに小さくなり、単純な積演算で評価可能。  

#### 5-4-3
外積による近似を用いて、ヘッセ行列の逆行列を近似するための効率的な計算方法を説明。  

#### 5-4-4
有限幅の差分による近似で、二回微分を求める。   
O(W^3)の計算量はO(W^2)の計算量に減らすことができる。  









