# PRMLのざっくりまとめ

## 1章
１章の内容まとめスライド  
https://www.slideshare.net/takushimiki/prml-52113785  
https://www.slideshare.net/matsuolab/prml1-78265686　　
機械学習全般の教師あり、教師なし、強化学習の大きな枠についての説明。  
演習問題の解答  
https://drive.google.com/drive/folders/0Bz9yuvZCp4qSZXB1MUpQSG9KQWs

### 1-1
簡単なsin(x)から生成したノイズいりの人工データから曲線フィッティングによる回帰を行う。  
観測データにはノイズが乗っており、与えられたxに対する目的変数tの値には不確実性がある。  
確率論は、不確実性を厳密に定量的に表現する。  
基底関数を多項式とする線形回帰を考える。  
線形モデルの係数値は予測値と目的変数の差を表す誤差関数（損失関数と同義？）を最小化して達成できる。  
単純で、1番使われている誤差関数は二乗話誤差。  
平均二乗平方根誤差RMSEの利点はNで割ることでサイズの異なるデータ集合を比較できる。目的変数tと同じ尺度（単位）であることが保証されること。  
回帰モデルの多項式の次数を増やすと、次数の増加に伴い、係数の値が増大し、訓練データに適合しすぎてしまう。  
最小二乗でモデルのパラメータを求めるのは、最尤推定での特別な場合に相当する。  
→過学習が最尤推定の一般的な性質である。  
過学習の問題を避けるために、ベイズ的アプローチが有効。  
正則化の概念について説明。  
二次の正則化＝Ridge回帰＝ニューラルネットワークの文脈で荷重減衰  

### 1-2
確率論の基礎的な概念の解説。  
確率の同時分布の対称性と、情報定理からベイズの定理が導かれる。  
事前確率：観測するより前にわかっている確率  
事後確率：一度事象が確認されてからベイズの定理から求められる、推定した事象の確率  
今まで直感的に使ってきた確率はランダムな繰り返しの頻度とみなされる、古典的確率、頻度主義的な確率解釈。
ベイズの定理を機械学習に当てはめるとすると  
p（w|X） = p(X|w)p(w) / p(X)  
パラメータの事後確率　＝　モデル・パラメータの事前分布 / 規格化定数  
事象xを確認した後に、wに関する不確実性を事後分布の形で評価する。  
不確実性を定量的に表現し、新たなデータで修正していく方法をベイズ的な確率解釈は実現可能。  
p(X|w)を尤度といい,尤度を最大化させる方法を最尤推定法という。  
ベイズの定理は  
事後確率 ∝ 尤度 × 事前確率  
最尤推定の気持ちとしては、データを生成する確率を最大にするパラメータがいいパラメータだろうという考え。  
最尤推定の問題点  
・尤度p(X|w)は厳密には確率ではない  
・単純に尤度を最大化すると過学習しやすい  
・モデル選択が難しい場合が多い。   
ベイズ的なアプローチの利点は、事前知識を、事前確率として自然にモデルに入れることができる点。  
公平に見えるコインを３回投げて毎回表がでたとしても、表がでる確率を頻度主義的な考えだと１になるのを防ぐことができる。  
マルコフ連鎖モンテカルロ法MCMC法のようなサンプリング法の開発がベイズの定理を実用化させてきた。  

#### 1-2-4
ガウス分布の性質についての内容。  
ガウス分布は、平均と分散によって定められ、全領域で積分すると１になるため確率分布の用件を満たす。  
ガウス分布での最頻値と平均は一致する。  
同一の正規分布から独立にデータがN個生成された場合のデータ集合の確率（尤度）は、正規分布の積で表される。  
最尤推定で求めたパラメータの期待値はE=N-1/N σ^2で、バイアスのある推定量  
データ点の数が増えればバイアスの影響はなくなる。データ量が多い理由の一つ。  

#### 1-2-5
最尤法を用いて曲線フィッティングを行う。  
ノイズが正規分布から発生していると考えると、モデルは正規分布の形で書くことができる。p(t|x,w,β) = N(t|y(x,w), β^-1)  
ここから全てのデータからの尤度を求めて、最尤推定法によりパラメータを求める。  
データに基づいた事後確率を最大にするパラメータ推定法を最大事後確率（MAP）推定という。  

#### 1-2-6
完全なベイズアプローチではwのすべての値に関して積分する必要がある。  
予測分布はガウス分布の形で与えられる。  

### 1-3
モデルの性能評価をまとめた内容  
交差分割検証:訓練データをs分割、訓練時間はs倍。  
情報量規準：赤池情報量規準AIC、ベイズ情報量規準BICを用いる。  

### 1-4
データをます目に分割すると、入力変数が増加するとマス目の数が指数関数的に増大する。  
多項式曲線フィッティングを複数個の入力変数に拡張した場合、より高次の多項式が必要となり、係数の数はべき乗に増える。  
N次元の球の体積は表面に近い薄皮部分に集中するという、幾何的直感と一致しない  
大きい異次元空有間に伴う困難全般のことを次元の呪いとよぶ  
実用では目的変数の変化を生じさせる方向は限られているため、意外と高次元でもなんとかなる。

### 1-5
クラス分類や、回帰予測の値の決定は誤識別率が最小になるように決定される。  
最大事後確率を
最大事後確率をもつクラスに決定される。  
識別率だけ上げると、問題になる分類もある。→損失関数を定めて、期待損失を最小化させる。  
決定問題を解く時の三つのアプローチ  

・生成モデル：　　
クラス事後確率p(C|x)、同時分布を推論する。出力の分布だけではなく、入力の分布もモデル化→モデルからのサンプリングで入力空間で人工データを生成できる。  
データの周辺分布p(x)を決めることができるため、低い確率をとる新しいデータ点をみつけ、外れ値検出、新規性検出に用いることができる。  
同時分布p(x,C)を推定するのにデータを大量に消費するため、事後確率p(C|x)のみ必要な場合は識別モデルを用いれば良い。  

・識別モデル：  
事後確率のみを推論して決定論でクラス割り当て。出力の分布をモデル化  

・識別関数：  
入力から直接クラスラベルに写像する関数f(x)をみつける。確率は用いいない。   

事後確率を求める利点：
訓練データの修正による事前確率の修正が容易、棄却オプションを変更可能、複数のモデルの結合が容易  

回帰の場合は、損失関数を二乗誤差とする。  
最適解は、xが与えられた下でのtの条件つき平均で、回帰関数といわれる。  

損失関数＝MSE＋目標データが持つノイズ  
と表され、損失関数の最初値はデータのノイズで決まる。  

### 1-6
離散確率変数xを観測した時の情報量 h(x) = -log{p(x)}
確率が低い事象が起こった時ほど大きな値をとる。
h(x)の単位はビット
情報量の平均をエントロピーと呼ぶ.

#### 1-6-1
真の分布p(x)をq(x)で近似する。q(x)でxの値を特定するのに追加で必要な情報量はKLダイバージェンスで求められる。  
KLダイバージェンス＝（p(x)のエントロピー）　ー　（q（x）のエントロピー)  
KL大バージェンンスの最小化＝尤度最大化  

## ２章
詳しい解説スライド  
https://www.slideshare.net/TakutoKimura/prml21-222425  
まとめスライド  
https://www.slideshare.net/matsuolab/prml2

演習問題の解答  
https://drive.google.com/drive/folders/0Bz9yuvZCp4qSY05fWDVqOHpzSmc  

有名な確率分布やその特徴をまとめた章。  
ベイズ推論をについても触れる。  
パラメトリック：  
ガウス分布の平均、分散など、少数のパラメータにより確率分布が決定される  
パラメタの値を決める手段は必要になり、そのうちの一つが尤度関数の最適化となる。  
ベイズ主義の立場では、パラメータに事前分布を導入して、観測データが与えられた場合のパラメータの事後分布をベイズの定理を用いて計算する。  

ノンパラメトリック：少数のパラメタでは確率分布が決まらない。一応パラメタはあるが、モデルの複雑さを決めるために使われる  

### 2-1
ベルヌーイ分布：コイン投げによる確率分布  
０がでる確率μのみでモデルが決定される。μについて尤度関数を微分して解くと、μの最尤推定量が求まる。  
＝サンプル平均sum(x)/N たまたま３回表が出た時に、最尤推定では必ず表がでるという推定結果になる。  
二項分布：コイン投げにおける表のでる回数  

#### 2-1-1
コイン投げの例のように最尤推定では、サンプル数が少ないと過学習を起こしやすいため、事前分布を考えるベイズ主義的に扱う。  
妥当なモデルを事前分布に採用←妥当性は検証する必要あり  
ベルヌーイ分布のパラメタμの事前分布にベータ分布を用いる。←共役性のため  
逐次学習：  
新たなサンプルが追加されることで、尤度関数を更新し、事後分布が更新される方法  

### 2-2
ベルヌーイ分布、二項分布の他変数への拡張を行なっている。＝多項分布  
他変量の場合の対数尤度の最大化はラグランジュの未定乗数法を用いて行える。
多項分布のパラメタμkの事前分布としてディリクレ分布を採用  

### 2-3
ガウス分布についての内容。  
複数の確率変数の和の確率分布はガウス分布になる(中心極限定理)  
確率変数が互いに独立であるこが必要  
ガウス分布はマハラノビス距離を通して、xに依存する。  
ガウス分布は単峰形（極大値が一つ）という条件から、多峰形の分布をうまく近似できない。  
→潜在変数を導入することで解決。  

#### 2-3-1
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、条件付き分布p(xa|xb) もガウス分布に従う  
→どのように同時分布の断面を切ってもガウス分布
この証明を説明している。  
二つの確率変数集合の同時分布p(xa,ab)がガウス分布に従う時、周辺分布p(xa), p(xb)もガウス分布に従う。  
やっている内容は、ベクトルxを二つの部分ベクトルx = (xa,xb)に分割したガウス分布p(x）を考え、条件付き分布p(xa|xb)と周辺分布p(xa)を求めている。  

#### 2-3-3
ガウス分布における変数で、ベイズの定理的理解をする節
ガウス周辺分布p(x)と平均がxの線型関数で共分散はxとは独立であるガウス条件付き分布p(y|x)を定義している。  
＝線型ガウスモデル  

#### 2-3-4
多変量ガウス分布から標本Xが得られた時、母集団の多変量ガウス分布のパラメタを最尤推定で推定できる。  
対数尤度関数は、ガウス分布の十分統計量にのみ依存している。  
最尤推定による平均は、データXの平均と一致し、共分散も通常の共分散の平均を最尤推定に置き換えたものになる。  

#### 2-3-5
ガウス分布の最尤推定において、逐次推定を行う。  
誤差信号の方向へ推定量を更新。  
Robbins-Monroアルゴリズム＝一般的な最尤推定問題を逐次的に解く方法。  

#### 2-3-6
ガウス分布における最尤推定の枠組みで、パラメータ上の事前分布を導入してベイズ主義的な扱いをする。  
観測データが生じる確率である尤度関数は、平均μの関数とみなせる。  
事前分布p(μ）にガウス分布を選べば、尤度関数の共役事前分布となる。  
平均を既知として、分散を推定する場合は共役事前分布ガンマ分布を用いる。  
平均も分散も未知として推定する場合は、事前分布に正規-ガンマ分布を考える  

#### 2-3-7
スチューデントのt分布についての説明。  
μ＝１でコーシー分布となる。  
平均は同じで、精度の異なるようなガウス分布を無限個足し合わせたもの。  
ガウス分布に比べ裾が広く、外れ値に対してロバストである。  
t分布に対する最尤推定会はEMアルゴリズムによって求まる、  

#### 2-3-8
ガウス分布に周期変数を導入する。  
周期変数の観測値の集合は曲座標表示可能。  
フォンミーゼス分布：確率変数、統計量を極座標表示することで、ガウス分布の周期変数への一般化を行なっている分布、  

#### 2-3-9
混合ガウス分布：ガウス分布を線形結合することでタータの分布を表現する  
線型結合する重みの係数と各分布の平均と共分散を調節すれば、任意の連続な密度関数を任意の精度で近似できる。  
混合係数は確率の条件を満たしており、k番目の混合要素を選択する事前確率と捉えられる。
事後確率p(k|x)＝負担率とする  
最尤推定は解析解が難しい→EMアルゴリズムの出番  

### 2-4
指数型分布族の全般的な性質について説明した内容  
→ベルヌーイ分布、多項分布、ガウス分布  
ベルヌーイ分布の確率分布からシグモイド関数を導ける。  
多項分布は二項分布の一般化版。  
最尤推定の解は十分推定量のみによって決まる。  

#### 2-4-2
共役事前分布についての一般的な説明。  
ある確率分布p(x|λ)について、事後分布がその事前分布と同じになる関数系になる、尤度関数と共役な事前分布p(λ)を求めることは可能。  

#### 2-4-3
無情報事前分布：事前分布がわからない時に、できるだけ事後分布に影響を及ぼさないような分布。  
並行移動不変性、尺度不変性がある。  

#### 2-4-5
パラメトリック法：  
少数のパラメタから確率変数の分布の形状を決める。  
あらかじめ確率分布の形状を仮定する。  
仮定した分布が不適切だと、予測性能が悪くなる。  

この節ではノンパラメトリック方について扱っている。  
分布の形状に制限されず、データによって形状を決定する。  
・ヒストグラム密度推定法  
連続値をヒストグラム化させ、データ集合を元にした分布を作成する。  
区間の幅が大きすぎるとモデルがなだらかになりすぎて、うまく分布の特性を捉えられない。  
いったん推定すれば、データを破棄できるため大規模データに有効  
・カーネル密度推定法  
観測点xを含む微小領域Rを考え、R内の点の総数Kが二項分布に従うことを用いる。  

・最近傍法  
カーネル密度推定ではカーネル幅を決めるパラメータhがすべてのカーネルで一定という問題を解決した手法。  

## 3章
線形回帰モデルを解説した章  
入力xに対し的固定された基底関数の線形結合をとる。  
パラメータに対しては線型で、入力変数に対しては非線形になりうる。  
確率論的には：  
xにたいするtの値の不確かさを表すために予測分布p(t|x)をモデル化  
損失関数の期待値を最小化するように任意のxの新しい値に対するtを予測可能。   

解説スライド  
https://www.slideshare.net/sotetsukoyamada/prml3332  
https://www.slideshare.net/matsuolab/prml3-78265931  

### 3-1
線型基底関数モデル：最も単純な線形回帰モデル。入力変数の線形結合y=sum(wx)  
パラメータwに対して線型かつ入力xに対しても線型  
→表現力が乏しいので、非線形な基底関数の線形結合を考える。y=sum(wφ(x))  
パラメータwに関しては線型なので、線型モデルと呼ばれる。  
すでに考えた多項式回帰
多項式回帰では、入力空間の一部の変化が他の領域に影響を及ぼしてしまう問題を、入力空間を分割し、各領域で多項式を当てはめる＝スプライン関数  
基底関数の例に、多項式基底、ガウス基底、シグモイド基底があげられる。  
tanh関数はシグモイド関数で表されるので、シグモイド関数の線形結合とtanhの線形結合は等価。  
この章では、基底関数のとりかたに依存しないので、特に指定していない。  

#### 3-1-1
最小二乗法と最尤推定の関係を説明する節  
目的変数tを関数がガウスノイズの和で与えられる場合を考える。  
t = y(x,w)+ ε  
tはガウス分布で表される状況になる。p(t|x,w,β)=N(t|y(x,w),β^-1)  
この場合の尤度を考える。  
線型ガウスモデルの条件では、尤度最大化と二乗誤差最小化は等価  
対数尤度を最大化させると最小二乗法での正規方程式が得られる。  
バイアスw0は目標値の平均と基底関数の値の平均の重み付き和との差を埋め合わせる役割を持つ。  

#### 3-1-2
最小二乗法の幾何学的な解釈  
データ点からモデルの貼る空間への射影が最尤推定により定まる。  
2つ以上の基底ベクトルが線型従属の時、

#### 3-1-3
逐次的学習のおさらい。確率的勾配効果法SGDを説明。

#### 3-1-4
正則化項を加えた最小二乗法の説明  
正則化項は機械学習の分野では荷重減衰、統計学の分野ではパラメータ縮小推定と呼ばれる。    
lassoがスパースになりやすい理由を図的に説明  

#### 3-1-5
目標ベクトルtの様子に対して異なる基底関数を用いれば、多次元出力の問題は、一次元回帰問題に帰着される。  
実際に使われているアプローチは、同じ基底関数を目標ベクトルの全ての要素に対してモデル化するもの  

### 3-2
最尤推定、最小二乗法の問題：  
過学習をしやすいので、基底関数の数を減らす→モデルの表現力が減る  
正則化項の導入→正則化係数λの決定が難しい。  
ベイズ的にパラメータを周辺化すれば解決する。  
この節はバイアス、バリアンスのトレードオフと知られるモデルの複雑さを説明  

予測yの評価をしたい期待二乗損失はbias^2 + variance + noiseに分解できる。  
もし、無数のデータが利用可能なら、原理的には任意の精度の回帰関数を求められるが、現実には有限データのみしか扱えないので不可能  
学習アルゴリズムの性能はデータ集合のとり方に関して平均することで評価する。  
真の分布と、回帰関数の期待二乗誤差は、バイアスの二乗とバリアンスの和で表される。  
バイアス：全てのデータ集合のとりかたに関する予測値の平均が理想的な回帰関数からどれくらい離れているかを表す。  
バリアンス：データが変わると、予測値がどれだけ変わるかを表す。  
学習の目標は、二乗バイアス、バリアンス、ノイズだけに依存する期待損失を最小化すること。  
ノイズは、真の予測分布がわからないと行けないので、求めることができない。  
バイアス大、バリアンス小：柔軟性のある複雑なモデル  
バイアス小、バリアンス大：柔軟性の低い  

### 3-3
最尤推定：  
モデルの複雑さをデータに依存して決める。正則化項でモデルの複雑さを調整。過学習の可能性が高い。訓練データの一部をテストデータにするのがもったいない。  
ベイズ線形回帰：  
パラメータを確率変数として扱う。過学習を回避できる。訓練データだけで、モデルの複雑さを推定可能  

#### 3-3-1
線形回帰モデルの尤度関数はwの二次関数の指数なので、モデルパラメータwの事前分布をガウス分布として考える。(ベイズ的発想)  
事後分布は尤度関数×事前分布なので、ガウス分布となる。  
事後確率はガウス分布なので、最頻値は期待値と一致し、事後確率を最大にする重みベクトルは単純に期待値となる。  
議論を簡単にするために、期待値０で単一精度パラメータαで表される等方的ガウス分布を考える。＝wの事前分布  
事後分布の対数をwに関して最大化するには、二乗誤差関数と二次正則化項の和を最小にすることと等価。  
y=w0+w1xの1次元の線型モデルを考える。  
データからパラメータを復元し、データのサイズと推定値との関係を明らかにする。  
逐次更新：初期値を適当に取り出して尤度関数を求める。尤度関数と事前分布をかけて事後確率を求める。事後確率から適当にパラメータを決め、関数を推定。以上を繰り返す。 

#### 3-3-2
実際の場面ではwの値そのものより、新しいxに対するtを予測したい。  
予測分布の評価をする必要がある。  
予測分布の分散は、新しいデータが追加されると事後分布は狭くなっていく。  
予測分散＝データに含まれるノイズ＋wに関する不確かさ  

#### 3-3-3
線型基底関数モデルに対して、事後分布の平均解を求めると、点xでの予測分布の平均は訓練データの目標変数tの線型結合になる。  
y=sum(k(x,x)t)  
k(x,x)を等価カーネルと呼び、訓練データの目標値の線型結合をとることで予測を行う回帰関数を線型平滑器とよぶ   
基底関数













