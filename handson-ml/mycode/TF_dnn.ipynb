{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-756fcc29cdc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhidden1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"hidden1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9048\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9656\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9774\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9782\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "# batch normalizationの重みを保存\n",
    "training = tf.placeholder_with_default(False, shape=(),name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
    "\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn_act, n_hidden2, name='hidden2')\n",
    "\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training,momentum=0.9)\n",
    "bn_act2 = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn =tf.layers.dense(bn_act2, n_outputs, name='outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8952\n",
      "1 Validation accuracy: 0.9202\n",
      "2 Validation accuracy: 0.9318\n",
      "3 Validation accuracy: 0.9422\n",
      "4 Validation accuracy: 0.9468\n",
      "5 Validation accuracy: 0.954\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.96\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9638\n",
      "10 Validation accuracy: 0.9662\n",
      "11 Validation accuracy: 0.9682\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9726\n",
      "18 Validation accuracy: 0.9738\n",
      "19 Validation accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7944\n",
      "2 Validation accuracy: 0.8796\n",
      "3 Validation accuracy: 0.906\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.9218\n",
      "6 Validation accuracy: 0.9292\n",
      "7 Validation accuracy: 0.936\n",
      "8 Validation accuracy: 0.9382\n",
      "9 Validation accuracy: 0.9414\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.947\n",
      "12 Validation accuracy: 0.9476\n",
      "13 Validation accuracy: 0.953\n",
      "14 Validation accuracy: 0.9566\n",
      "15 Validation accuracy: 0.9568\n",
      "16 Validation accuracy: 0.9578\n",
      "17 Validation accuracy: 0.9594\n",
      "18 Validation accuracy: 0.9624\n",
      "19 Validation accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph('./my_model_final.ckpt.meta')\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy:0')\n",
    "training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')\n",
    "\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-42-86d8dd407eef>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.964\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.965\n",
      "4 Validation accuracy: 0.9642\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.9688\n",
      "7 Validation accuracy: 0.9688\n",
      "8 Validation accuracy: 0.9684\n",
      "9 Validation accuracy: 0.9682\n",
      "10 Validation accuracy: 0.9706\n",
      "11 Validation accuracy: 0.9712\n",
      "12 Validation accuracy: 0.967\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9724\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9712\n",
      "18 Validation accuracy: 0.9712\n",
      "19 Validation accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 =10\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.] \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernl = graph.get_operation_by_name('hidden/kernel/Assign')\n",
    "assign_bias = graph.get_operation_by_name('hidden/kernel/Assign')\n",
    "init_kernel = assign_kernl.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel:original_w, init_bias:original_b})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name='hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4')\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # 重みを凍結する層の選択\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|outputs')\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value outputs/bias\n\t [[node outputs/bias/read (defined at <ipython-input-53-832a2310deff>:19) ]]\n\nCaused by op 'outputs/bias/read', defined at:\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-832a2310deff>\", line 19, in <module>\n    logits = tf.layers.dense(hidden4, n_outputs, name='outputs')\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 188, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1227, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 538, in __call__\n    self._maybe_build(inputs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1603, in _maybe_build\n    self.build(input_shapes)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 958, in build\n    trainable=True)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 435, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 349, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 607, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3890, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value outputs/bias\n\t [[node outputs/bias/read (defined at <ipython-input-53-832a2310deff>:19) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value outputs/bias\n\t [[{{node outputs/bias/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-fb18e285efb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshuffle_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Validation accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value outputs/bias\n\t [[node outputs/bias/read (defined at <ipython-input-53-832a2310deff>:19) ]]\n\nCaused by op 'outputs/bias/read', defined at:\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-832a2310deff>\", line 19, in <module>\n    logits = tf.layers.dense(hidden4, n_outputs, name='outputs')\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 188, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1227, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 538, in __call__\n    self._maybe_build(inputs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1603, in _maybe_build\n    self.build(input_shapes)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 958, in build\n    trainable=True)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 435, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 349, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 607, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3890, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value outputs/bias\n\t [[node outputs/bias/read (defined at <ipython-input-53-832a2310deff>:19) ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='hidden[123]')\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train,batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9196\n",
      "1 Validation accuracy: 0.9394\n",
      "2 Validation accuracy: 0.949\n",
      "3 Validation accuracy: 0.953\n",
      "4 Validation accuracy: 0.9554\n",
      "5 Validation accuracy: 0.9558\n",
      "6 Validation accuracy: 0.9578\n",
      "7 Validation accuracy: 0.961\n",
      "8 Validation accuracy: 0.9614\n",
      "9 Validation accuracy: 0.9642\n",
      "10 Validation accuracy: 0.9652\n",
      "11 Validation accuracy: 0.9658\n",
      "12 Validation accuracy: 0.9638\n",
      "13 Validation accuracy: 0.967\n",
      "14 Validation accuracy: 0.968\n",
      "15 Validation accuracy: 0.9682\n",
      "16 Validation accuracy: 0.9698\n",
      "17 Validation accuracy: 0.9676\n",
      "18 Validation accuracy: 0.9694\n",
      "19 Validation accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_neterov=True)\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate-learning_rate)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,momentum=0.9,decay=0.9,epsilon=1e-10)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.97\n",
      "1 Validation accuracy: 0.9722\n",
      "2 Validation accuracy: 0.9758\n",
      "3 Validation accuracy: 0.9828\n",
      "4 Validation accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    # 正則化項を加える\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9066\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name='avd_xentropy')\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name='loss')\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.908\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9124\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n",
      "10 Validation accuracy: 0.92\n",
      "11 Validation accuracy: 0.9224\n",
      "12 Validation accuracy: 0.9212\n",
      "13 Validation accuracy: 0.9228\n",
      "14 Validation accuracy: 0.9222\n",
      "15 Validation accuracy: 0.9218\n",
      "16 Validation accuracy: 0.9218\n",
      "17 Validation accuracy: 0.9228\n",
      "18 Validation accuracy: 0.9216\n",
      "19 Validation accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-66-3f52e26d4375>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    \n",
    "    # dropout\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    # dropout\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9264\n",
      "1 Validation accuracy: 0.9464\n",
      "2 Validation accuracy: 0.9518\n",
      "3 Validation accuracy: 0.9554\n",
      "4 Validation accuracy: 0.9586\n",
      "5 Validation accuracy: 0.963\n",
      "6 Validation accuracy: 0.9618\n",
      "7 Validation accuracy: 0.9664\n",
      "8 Validation accuracy: 0.968\n",
      "9 Validation accuracy: 0.9702\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.9708\n",
      "12 Validation accuracy: 0.97\n",
      "13 Validation accuracy: 0.9718\n",
      "14 Validation accuracy: 0.9748\n",
      "15 Validation accuracy: 0.9706\n",
      "16 Validation accuracy: 0.9732\n",
      "17 Validation accuracy: 0.9718\n",
      "18 Validation accuracy: 0.973\n",
      "19 Validation accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# 重み上限正則化\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name('hidden2/kernel:0')\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9566\n",
      "1 Validation accuracy: 0.9704\n",
      "2 Validation accuracy: 0.972\n",
      "3 Validation accuracy: 0.977\n",
      "4 Validation accuracy: 0.9768\n",
      "5 Validation accuracy: 0.977\n",
      "6 Validation accuracy: 0.9822\n",
      "7 Validation accuracy: 0.9824\n",
      "8 Validation accuracy: 0.979\n",
      "9 Validation accuracy: 0.9822\n",
      "10 Validation accuracy: 0.9818\n",
      "11 Validation accuracy: 0.9834\n",
      "12 Validation accuracy: 0.9812\n",
      "13 Validation accuracy: 0.9832\n",
      "14 Validation accuracy: 0.9842\n",
      "15 Validation accuracy: 0.984\n",
      "16 Validation accuracy: 0.9832\n",
      "17 Validation accuracy: 0.984\n",
      "18 Validation accuracy: 0.9844\n",
      "19 Validation accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9558\n",
      "1 Validation accuracy: 0.97\n",
      "2 Validation accuracy: 0.9724\n",
      "3 Validation accuracy: 0.9754\n",
      "4 Validation accuracy: 0.9772\n",
      "5 Validation accuracy: 0.9788\n",
      "6 Validation accuracy: 0.981\n",
      "7 Validation accuracy: 0.9812\n",
      "8 Validation accuracy: 0.9814\n",
      "9 Validation accuracy: 0.9812\n",
      "10 Validation accuracy: 0.9814\n",
      "11 Validation accuracy: 0.9814\n",
      "12 Validation accuracy: 0.981\n",
      "13 Validation accuracy: 0.9828\n",
      "14 Validation accuracy: 0.9818\n",
      "15 Validation accuracy: 0.983\n",
      "16 Validation accuracy: 0.9818\n",
      "17 Validation accuracy: 0.9832\n",
      "18 Validation accuracy: 0.9822\n",
      "19 Validation accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_int = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, activation=tf.nn.elu, initializer=he_int):\n",
    "# variable_scope で変数を階層化\n",
    "    with tf.variable_scope(name, 'dnn'):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation, \n",
    "                                     kernel_initializer=initializer, \n",
    "                                     name='hidden%d' % (layer+1))\n",
    "        return inputs\n",
    "\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_int, name='logits')\n",
    "Y_proba = tf.nn.softmax(logits, name='Y_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss, name='training_op')\n",
    "\n",
    "# in_top_k(y_pred, y_true, k)でy_predの上位k番目までの中に真値が入っていた場合、trueになる。\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# tf.castで型変換を一発で行う\n",
    "# tf.reduce_mean 与えたリストに入っている数値の平均値を求める関数\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの削減\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.378996\tBest loss: 0.378996\tAccuracy: 90.54%\n",
      "1\tValidation loss: 0.110911\tBest loss: 0.110911\tAccuracy: 97.22%\n",
      "2\tValidation loss: 0.219973\tBest loss: 0.110911\tAccuracy: 95.97%\n",
      "3\tValidation loss: 0.240473\tBest loss: 0.110911\tAccuracy: 91.05%\n",
      "4\tValidation loss: 0.113470\tBest loss: 0.110911\tAccuracy: 97.62%\n",
      "5\tValidation loss: 0.150360\tBest loss: 0.110911\tAccuracy: 96.36%\n",
      "6\tValidation loss: 0.793285\tBest loss: 0.110911\tAccuracy: 59.77%\n",
      "7\tValidation loss: 0.604311\tBest loss: 0.110911\tAccuracy: 90.19%\n",
      "8\tValidation loss: 0.380075\tBest loss: 0.110911\tAccuracy: 92.69%\n",
      "9\tValidation loss: 0.332835\tBest loss: 0.110911\tAccuracy: 94.41%\n",
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 97.55%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    # ランダムに学習データを選ぶ\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "    # 学習training_opを実行\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    # loss ,accuracyをX,yに検証データを入れて求める\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X:X_valid1, y:y_valid1})\n",
    "    # 早期打ち切りを付ける\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, 'ch11/my_mnist_model_0_to_4.ckpt')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        \n",
    "        else:\n",
    "            checks_without_progress +=1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('early stopping')\n",
    "                break\n",
    "                \n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'ch11/my_mnist_model_0_to_4.ckpt')\n",
    "    acc_test = sess.run(accuracy, feed_dict={X:X_test1, y:y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # 入力データのプレースホルダー\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        # DNNの構築\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            # get_collection で特定のスコープを取り出す\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.122529\tBest loss: 0.122529\tAccuracy: 96.79%\n",
      "1\tValidation loss: 0.162101\tBest loss: 0.122529\tAccuracy: 96.44%\n",
      "2\tValidation loss: 0.108512\tBest loss: 0.108512\tAccuracy: 97.38%\n",
      "3\tValidation loss: 0.089879\tBest loss: 0.089879\tAccuracy: 97.93%\n",
      "4\tValidation loss: 0.165096\tBest loss: 0.089879\tAccuracy: 96.48%\n",
      "5\tValidation loss: 0.137959\tBest loss: 0.089879\tAccuracy: 97.50%\n",
      "6\tValidation loss: 0.111896\tBest loss: 0.089879\tAccuracy: 97.54%\n",
      "7\tValidation loss: 0.130334\tBest loss: 0.089879\tAccuracy: 97.26%\n",
      "8\tValidation loss: 0.610585\tBest loss: 0.089879\tAccuracy: 78.34%\n",
      "9\tValidation loss: 0.322547\tBest loss: 0.089879\tAccuracy: 97.11%\n",
      "10\tValidation loss: 0.812121\tBest loss: 0.089879\tAccuracy: 92.22%\n",
      "11\tValidation loss: 1.363133\tBest loss: 0.089879\tAccuracy: 34.36%\n",
      "12\tValidation loss: 1.379614\tBest loss: 0.089879\tAccuracy: 38.15%\n",
      "13\tValidation loss: 1.362691\tBest loss: 0.089879\tAccuracy: 35.38%\n",
      "14\tValidation loss: 1.556379\tBest loss: 0.089879\tAccuracy: 26.82%\n",
      "15\tValidation loss: 1.374037\tBest loss: 0.089879\tAccuracy: 38.62%\n",
      "16\tValidation loss: 1.392678\tBest loss: 0.089879\tAccuracy: 34.13%\n",
      "17\tValidation loss: 1.328980\tBest loss: 0.089879\tAccuracy: 33.23%\n",
      "18\tValidation loss: 1.547397\tBest loss: 0.089879\tAccuracy: 23.53%\n",
      "19\tValidation loss: 1.294273\tBest loss: 0.089879\tAccuracy: 34.79%\n",
      "20\tValidation loss: 1.385632\tBest loss: 0.089879\tAccuracy: 36.40%\n",
      "21\tValidation loss: 1.413917\tBest loss: 0.089879\tAccuracy: 35.07%\n",
      "22\tValidation loss: 3.707740\tBest loss: 0.089879\tAccuracy: 24.08%\n",
      "23\tValidation loss: 1.268231\tBest loss: 0.089879\tAccuracy: 37.57%\n",
      "24\tValidation loss: 1.393786\tBest loss: 0.089879\tAccuracy: 36.79%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x000001C7E379E7B8>,\n",
       "              batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C8AF7574A8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785950574041642"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                cv=3, random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "\n",
    "# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n",
    "# fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "# rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                 fit_params=fit_params, random_state=42, verbose=2)\n",
    "# rnd_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.083823\tBest loss: 0.083823\tAccuracy: 97.50%\n",
      "1\tValidation loss: 0.055118\tBest loss: 0.055118\tAccuracy: 98.28%\n",
      "2\tValidation loss: 0.040665\tBest loss: 0.040665\tAccuracy: 98.75%\n",
      "3\tValidation loss: 0.056053\tBest loss: 0.040665\tAccuracy: 98.48%\n",
      "4\tValidation loss: 0.044132\tBest loss: 0.040665\tAccuracy: 98.48%\n",
      "5\tValidation loss: 0.037938\tBest loss: 0.037938\tAccuracy: 98.55%\n",
      "6\tValidation loss: 0.039892\tBest loss: 0.037938\tAccuracy: 98.94%\n",
      "7\tValidation loss: 0.051411\tBest loss: 0.037938\tAccuracy: 98.51%\n",
      "8\tValidation loss: 0.051926\tBest loss: 0.037938\tAccuracy: 98.91%\n",
      "9\tValidation loss: 0.063269\tBest loss: 0.037938\tAccuracy: 98.59%\n",
      "10\tValidation loss: 0.045566\tBest loss: 0.037938\tAccuracy: 98.83%\n",
      "11\tValidation loss: 0.062851\tBest loss: 0.037938\tAccuracy: 98.63%\n",
      "12\tValidation loss: 0.058805\tBest loss: 0.037938\tAccuracy: 98.75%\n",
      "13\tValidation loss: 0.065914\tBest loss: 0.037938\tAccuracy: 98.63%\n",
      "14\tValidation loss: 0.073791\tBest loss: 0.037938\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.067815\tBest loss: 0.037938\tAccuracy: 98.98%\n",
      "16\tValidation loss: 0.071563\tBest loss: 0.037938\tAccuracy: 98.83%\n",
      "17\tValidation loss: 0.060745\tBest loss: 0.037938\tAccuracy: 98.91%\n",
      "18\tValidation loss: 0.050755\tBest loss: 0.037938\tAccuracy: 98.79%\n",
      "19\tValidation loss: 0.049964\tBest loss: 0.037938\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.041585\tBest loss: 0.037938\tAccuracy: 98.91%\n",
      "21\tValidation loss: 24.137550\tBest loss: 0.037938\tAccuracy: 91.71%\n",
      "22\tValidation loss: 2.520187\tBest loss: 0.037938\tAccuracy: 97.03%\n",
      "23\tValidation loss: 0.805464\tBest loss: 0.037938\tAccuracy: 98.01%\n",
      "24\tValidation loss: 0.676472\tBest loss: 0.037938\tAccuracy: 97.65%\n",
      "25\tValidation loss: 0.471427\tBest loss: 0.037938\tAccuracy: 98.16%\n",
      "26\tValidation loss: 0.300784\tBest loss: 0.037938\tAccuracy: 98.44%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000001C8B22F6048>,\n",
       "              batch_norm_momentum=None, batch_size=500, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C8AF7574A8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=140,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "\n",
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9918272037361354"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-99-99bc6ad3b7ba>:32: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From C:\\Users\\nsats\\Anaconda3\\envs\\gpu-env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0\tValidation loss: 0.044999\tBest loss: 0.044999\tAccuracy: 98.71%\n",
      "1\tValidation loss: 0.038797\tBest loss: 0.038797\tAccuracy: 98.67%\n",
      "2\tValidation loss: 0.038604\tBest loss: 0.038604\tAccuracy: 98.91%\n",
      "3\tValidation loss: 0.064228\tBest loss: 0.038604\tAccuracy: 97.77%\n",
      "4\tValidation loss: 0.049275\tBest loss: 0.038604\tAccuracy: 98.51%\n",
      "5\tValidation loss: 0.029667\tBest loss: 0.029667\tAccuracy: 99.14%\n",
      "6\tValidation loss: 0.030725\tBest loss: 0.029667\tAccuracy: 99.18%\n",
      "7\tValidation loss: 0.038786\tBest loss: 0.029667\tAccuracy: 98.83%\n",
      "8\tValidation loss: 0.026140\tBest loss: 0.026140\tAccuracy: 99.14%\n",
      "9\tValidation loss: 0.029221\tBest loss: 0.026140\tAccuracy: 99.18%\n",
      "10\tValidation loss: 0.034903\tBest loss: 0.026140\tAccuracy: 99.02%\n",
      "11\tValidation loss: 0.044801\tBest loss: 0.026140\tAccuracy: 98.71%\n",
      "12\tValidation loss: 0.040226\tBest loss: 0.026140\tAccuracy: 99.02%\n",
      "13\tValidation loss: 0.043110\tBest loss: 0.026140\tAccuracy: 98.98%\n",
      "14\tValidation loss: 0.034806\tBest loss: 0.026140\tAccuracy: 98.94%\n",
      "15\tValidation loss: 0.027465\tBest loss: 0.026140\tAccuracy: 99.30%\n",
      "16\tValidation loss: 0.035076\tBest loss: 0.026140\tAccuracy: 99.18%\n",
      "17\tValidation loss: 0.042076\tBest loss: 0.026140\tAccuracy: 99.22%\n",
      "18\tValidation loss: 0.037084\tBest loss: 0.026140\tAccuracy: 98.98%\n",
      "19\tValidation loss: 0.033529\tBest loss: 0.026140\tAccuracy: 99.18%\n",
      "20\tValidation loss: 0.034524\tBest loss: 0.026140\tAccuracy: 99.30%\n",
      "21\tValidation loss: 0.042282\tBest loss: 0.026140\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.034373\tBest loss: 0.026140\tAccuracy: 99.18%\n",
      "23\tValidation loss: 0.026587\tBest loss: 0.026140\tAccuracy: 99.34%\n",
      "24\tValidation loss: 0.036488\tBest loss: 0.026140\tAccuracy: 99.10%\n",
      "25\tValidation loss: 0.032330\tBest loss: 0.026140\tAccuracy: 99.30%\n",
      "26\tValidation loss: 0.045659\tBest loss: 0.026140\tAccuracy: 98.94%\n",
      "27\tValidation loss: 0.028581\tBest loss: 0.026140\tAccuracy: 99.41%\n",
      "28\tValidation loss: 0.021058\tBest loss: 0.021058\tAccuracy: 99.57%\n",
      "29\tValidation loss: 0.022321\tBest loss: 0.021058\tAccuracy: 99.41%\n",
      "30\tValidation loss: 0.025039\tBest loss: 0.021058\tAccuracy: 99.45%\n",
      "31\tValidation loss: 0.030021\tBest loss: 0.021058\tAccuracy: 99.22%\n",
      "32\tValidation loss: 0.027748\tBest loss: 0.021058\tAccuracy: 99.45%\n",
      "33\tValidation loss: 0.036144\tBest loss: 0.021058\tAccuracy: 99.30%\n",
      "34\tValidation loss: 0.041058\tBest loss: 0.021058\tAccuracy: 99.18%\n",
      "35\tValidation loss: 0.039373\tBest loss: 0.021058\tAccuracy: 99.14%\n",
      "36\tValidation loss: 0.036274\tBest loss: 0.021058\tAccuracy: 99.10%\n",
      "37\tValidation loss: 0.047247\tBest loss: 0.021058\tAccuracy: 98.91%\n",
      "38\tValidation loss: 0.029504\tBest loss: 0.021058\tAccuracy: 99.37%\n",
      "39\tValidation loss: 0.026131\tBest loss: 0.021058\tAccuracy: 99.37%\n",
      "40\tValidation loss: 0.030207\tBest loss: 0.021058\tAccuracy: 99.53%\n",
      "41\tValidation loss: 0.032427\tBest loss: 0.021058\tAccuracy: 99.22%\n",
      "42\tValidation loss: 0.029885\tBest loss: 0.021058\tAccuracy: 99.49%\n",
      "43\tValidation loss: 0.042887\tBest loss: 0.021058\tAccuracy: 99.02%\n",
      "44\tValidation loss: 0.033375\tBest loss: 0.021058\tAccuracy: 99.10%\n",
      "45\tValidation loss: 0.039735\tBest loss: 0.021058\tAccuracy: 99.18%\n",
      "46\tValidation loss: 0.036614\tBest loss: 0.021058\tAccuracy: 99.30%\n",
      "47\tValidation loss: 0.043034\tBest loss: 0.021058\tAccuracy: 98.98%\n",
      "48\tValidation loss: 0.051087\tBest loss: 0.021058\tAccuracy: 99.10%\n",
      "49\tValidation loss: 0.046974\tBest loss: 0.021058\tAccuracy: 99.10%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000001C8BAAE9C80>,\n",
       "              batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C8AF7574A8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933839268340144"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.132864\tBest loss: 0.132864\tAccuracy: 96.64%\n",
      "1\tValidation loss: 0.094044\tBest loss: 0.094044\tAccuracy: 97.11%\n",
      "2\tValidation loss: 0.096225\tBest loss: 0.094044\tAccuracy: 97.65%\n",
      "3\tValidation loss: 0.092083\tBest loss: 0.092083\tAccuracy: 97.69%\n",
      "4\tValidation loss: 0.079092\tBest loss: 0.079092\tAccuracy: 97.85%\n",
      "5\tValidation loss: 0.070968\tBest loss: 0.070968\tAccuracy: 97.93%\n",
      "6\tValidation loss: 0.073465\tBest loss: 0.070968\tAccuracy: 98.24%\n",
      "7\tValidation loss: 0.075080\tBest loss: 0.070968\tAccuracy: 98.20%\n",
      "8\tValidation loss: 0.076732\tBest loss: 0.070968\tAccuracy: 98.08%\n",
      "9\tValidation loss: 0.074943\tBest loss: 0.070968\tAccuracy: 98.44%\n",
      "10\tValidation loss: 0.073864\tBest loss: 0.070968\tAccuracy: 98.01%\n",
      "11\tValidation loss: 0.074112\tBest loss: 0.070968\tAccuracy: 98.08%\n",
      "12\tValidation loss: 0.065631\tBest loss: 0.065631\tAccuracy: 98.24%\n",
      "13\tValidation loss: 0.063012\tBest loss: 0.063012\tAccuracy: 98.32%\n",
      "14\tValidation loss: 0.065266\tBest loss: 0.063012\tAccuracy: 98.24%\n",
      "15\tValidation loss: 0.065009\tBest loss: 0.063012\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.067316\tBest loss: 0.063012\tAccuracy: 98.24%\n",
      "17\tValidation loss: 0.060679\tBest loss: 0.060679\tAccuracy: 98.59%\n",
      "18\tValidation loss: 0.068288\tBest loss: 0.060679\tAccuracy: 98.28%\n",
      "19\tValidation loss: 0.065420\tBest loss: 0.060679\tAccuracy: 98.40%\n",
      "20\tValidation loss: 0.082427\tBest loss: 0.060679\tAccuracy: 98.24%\n",
      "21\tValidation loss: 0.064835\tBest loss: 0.060679\tAccuracy: 98.36%\n",
      "22\tValidation loss: 0.069319\tBest loss: 0.060679\tAccuracy: 98.36%\n",
      "23\tValidation loss: 0.071750\tBest loss: 0.060679\tAccuracy: 98.28%\n",
      "24\tValidation loss: 0.066483\tBest loss: 0.060679\tAccuracy: 98.40%\n",
      "25\tValidation loss: 0.072288\tBest loss: 0.060679\tAccuracy: 98.32%\n",
      "26\tValidation loss: 0.078605\tBest loss: 0.060679\tAccuracy: 98.08%\n",
      "27\tValidation loss: 0.082328\tBest loss: 0.060679\tAccuracy: 98.12%\n",
      "28\tValidation loss: 0.078550\tBest loss: 0.060679\tAccuracy: 98.08%\n",
      "29\tValidation loss: 0.083569\tBest loss: 0.060679\tAccuracy: 97.85%\n",
      "30\tValidation loss: 0.110064\tBest loss: 0.060679\tAccuracy: 97.50%\n",
      "31\tValidation loss: 0.120066\tBest loss: 0.060679\tAccuracy: 97.07%\n",
      "32\tValidation loss: 0.128827\tBest loss: 0.060679\tAccuracy: 96.48%\n",
      "33\tValidation loss: 0.131391\tBest loss: 0.060679\tAccuracy: 95.47%\n",
      "34\tValidation loss: 0.127139\tBest loss: 0.060679\tAccuracy: 96.60%\n",
      "35\tValidation loss: 0.119193\tBest loss: 0.060679\tAccuracy: 96.76%\n",
      "36\tValidation loss: 0.111741\tBest loss: 0.060679\tAccuracy: 96.83%\n",
      "37\tValidation loss: 0.123720\tBest loss: 0.060679\tAccuracy: 96.48%\n",
      "38\tValidation loss: 0.145619\tBest loss: 0.060679\tAccuracy: 96.56%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000001C8BABB7BF8>,\n",
       "              batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C8AF7574A8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856003113446196"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph('ch11/my_mnist_model_0_to_4.ckpt.meta')\n",
    "    \n",
    "# get_tensor_by_name 特定のTensorを取得する\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y =tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "loss = tf.get_default_graph().get_tensor_by_name('loss:0')\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name('Y_proba:0')\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('accuracy:0')\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# get_collectionで特定の名前スコープに対して操作を行える。GraphKeys.TRAINABLE_VARIABLES で訓練可能な変数と設定\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='logits')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name='Adam2')\n",
    "# var_listでオプティマイザに更新する変数のリストを渡せる\n",
    "training_op = optimizer.minimize(loss,var_list=output_layer_vars)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_0_to_4.ckpt\n",
      "0\tValidation loss: 2.073083\tBest loss: 2.073083\tAccuracy: 94.41%\n",
      "1\tValidation loss: 1.616588\tBest loss: 1.616588\tAccuracy: 94.41%\n",
      "2\tValidation loss: 1.433028\tBest loss: 1.433028\tAccuracy: 94.41%\n",
      "3\tValidation loss: 1.334913\tBest loss: 1.334913\tAccuracy: 94.41%\n",
      "4\tValidation loss: 1.286134\tBest loss: 1.286134\tAccuracy: 94.41%\n",
      "5\tValidation loss: 1.472596\tBest loss: 1.286134\tAccuracy: 94.41%\n",
      "6\tValidation loss: 1.281950\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "7\tValidation loss: 1.319315\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "8\tValidation loss: 1.664335\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "9\tValidation loss: 1.409579\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "10\tValidation loss: 1.294340\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "11\tValidation loss: 1.358120\tBest loss: 1.281950\tAccuracy: 94.41%\n",
      "12\tValidation loss: 1.264460\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "13\tValidation loss: 1.327769\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "14\tValidation loss: 1.474794\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "15\tValidation loss: 1.284211\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "16\tValidation loss: 1.485086\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "17\tValidation loss: 1.384919\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "18\tValidation loss: 1.312274\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "19\tValidation loss: 1.346191\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "20\tValidation loss: 1.335120\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "21\tValidation loss: 1.590003\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "22\tValidation loss: 1.337094\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "23\tValidation loss: 1.297675\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "24\tValidation loss: 1.326304\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "25\tValidation loss: 1.401573\tBest loss: 1.264460\tAccuracy: 94.41%\n",
      "26\tValidation loss: 1.261881\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "27\tValidation loss: 1.305159\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "28\tValidation loss: 1.449578\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "29\tValidation loss: 1.309870\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "30\tValidation loss: 1.437256\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "31\tValidation loss: 1.534460\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "32\tValidation loss: 1.344028\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "33\tValidation loss: 1.349892\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "34\tValidation loss: 1.350740\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "35\tValidation loss: 1.318938\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "36\tValidation loss: 1.446561\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "37\tValidation loss: 1.396443\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "38\tValidation loss: 1.346661\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "39\tValidation loss: 1.293723\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "40\tValidation loss: 1.401663\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "41\tValidation loss: 1.446504\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "42\tValidation loss: 1.378359\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "43\tValidation loss: 1.508828\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "44\tValidation loss: 1.460468\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "45\tValidation loss: 1.307876\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "46\tValidation loss: 1.293004\tBest loss: 1.261881\tAccuracy: 94.41%\n",
      "early stopping\n",
      "Total training time: 5.7s\n",
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_5_to_9_five_frozen\n",
      "final accuracy: 47.91%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_epochs = 100\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # 変数を呼び込む\n",
    "    restore_saver.restore(sess, 'ch11/my_mnist_model_0_to_4.ckpt')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2)//batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        loss_val, loss_acc = sess.run([loss,accuracy], feed_dict={X:X_valid2, y:y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, 'ch11/my_mnist_model_5_to_9_five_frozen')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('early stopping')\n",
    "                break\n",
    "        \n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print('Total training time: {:.1f}s'.format(t1-t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, 'ch11/my_mnist_model_5_to_9_five_frozen')\n",
    "    acc_test = accuracy.eval(feed_dict={X:X_test2, y:y_test2})\n",
    "    print('final accuracy: {:.2f}%'.format(acc_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden5/Elu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_outputs = 5\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"ch11/my_mnist_model_0_to_4.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "#  4層目の出力を取得\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Elu:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 隠れ層４層目と出力層を訓練可能にする\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_0_to_4.ckpt\n",
      "0\tValidation loss: 2.789467\tBest loss: 2.789467\tAccuracy: 47.33%\n",
      "1\tValidation loss: 1.841614\tBest loss: 1.841614\tAccuracy: 46.00%\n",
      "2\tValidation loss: 1.936504\tBest loss: 1.841614\tAccuracy: 45.33%\n",
      "3\tValidation loss: 1.454504\tBest loss: 1.454504\tAccuracy: 52.00%\n",
      "4\tValidation loss: 1.558626\tBest loss: 1.454504\tAccuracy: 44.67%\n",
      "5\tValidation loss: 1.478964\tBest loss: 1.454504\tAccuracy: 45.33%\n",
      "6\tValidation loss: 1.414275\tBest loss: 1.414275\tAccuracy: 53.33%\n",
      "7\tValidation loss: 1.208015\tBest loss: 1.208015\tAccuracy: 51.33%\n",
      "8\tValidation loss: 2.046711\tBest loss: 1.208015\tAccuracy: 32.00%\n",
      "9\tValidation loss: 1.574036\tBest loss: 1.208015\tAccuracy: 40.67%\n",
      "10\tValidation loss: 1.832989\tBest loss: 1.208015\tAccuracy: 45.33%\n",
      "11\tValidation loss: 1.845016\tBest loss: 1.208015\tAccuracy: 40.67%\n",
      "12\tValidation loss: 1.386157\tBest loss: 1.208015\tAccuracy: 46.00%\n",
      "13\tValidation loss: 1.461256\tBest loss: 1.208015\tAccuracy: 43.33%\n",
      "14\tValidation loss: 1.462357\tBest loss: 1.208015\tAccuracy: 47.33%\n",
      "15\tValidation loss: 1.174797\tBest loss: 1.174797\tAccuracy: 49.33%\n",
      "16\tValidation loss: 1.390507\tBest loss: 1.174797\tAccuracy: 46.67%\n",
      "17\tValidation loss: 1.368200\tBest loss: 1.174797\tAccuracy: 45.33%\n",
      "18\tValidation loss: 1.285747\tBest loss: 1.174797\tAccuracy: 44.00%\n",
      "19\tValidation loss: 1.261043\tBest loss: 1.174797\tAccuracy: 47.33%\n",
      "20\tValidation loss: 1.849110\tBest loss: 1.174797\tAccuracy: 46.00%\n",
      "21\tValidation loss: 1.658534\tBest loss: 1.174797\tAccuracy: 39.33%\n",
      "22\tValidation loss: 1.270393\tBest loss: 1.174797\tAccuracy: 53.33%\n",
      "23\tValidation loss: 1.498666\tBest loss: 1.174797\tAccuracy: 43.33%\n",
      "24\tValidation loss: 1.637108\tBest loss: 1.174797\tAccuracy: 44.00%\n",
      "25\tValidation loss: 1.467383\tBest loss: 1.174797\tAccuracy: 47.33%\n",
      "26\tValidation loss: 1.517324\tBest loss: 1.174797\tAccuracy: 35.33%\n",
      "27\tValidation loss: 1.481824\tBest loss: 1.174797\tAccuracy: 46.67%\n",
      "28\tValidation loss: 2.303794\tBest loss: 1.174797\tAccuracy: 42.00%\n",
      "29\tValidation loss: 1.431568\tBest loss: 1.174797\tAccuracy: 44.67%\n",
      "30\tValidation loss: 1.410705\tBest loss: 1.174797\tAccuracy: 41.33%\n",
      "31\tValidation loss: 1.413884\tBest loss: 1.174797\tAccuracy: 36.67%\n",
      "32\tValidation loss: 1.310212\tBest loss: 1.174797\tAccuracy: 46.00%\n",
      "33\tValidation loss: 1.423988\tBest loss: 1.174797\tAccuracy: 39.33%\n",
      "34\tValidation loss: 1.342891\tBest loss: 1.174797\tAccuracy: 48.00%\n",
      "35\tValidation loss: 1.531401\tBest loss: 1.174797\tAccuracy: 47.33%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 47.81%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"ch11/my_mnist_model_0_to_4.ckpt\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"ch11/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"ch11/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# 隠れ層４層目と出力層を訓練可能にする\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_0_to_4.ckpt\n",
      "0\tValidation loss: 2.838898\tBest loss: 2.838898\tAccuracy: 37.33%\n",
      "1\tValidation loss: 1.837588\tBest loss: 1.837588\tAccuracy: 52.00%\n",
      "2\tValidation loss: 1.526819\tBest loss: 1.526819\tAccuracy: 54.67%\n",
      "3\tValidation loss: 1.457405\tBest loss: 1.457405\tAccuracy: 50.00%\n",
      "4\tValidation loss: 1.732809\tBest loss: 1.457405\tAccuracy: 48.00%\n",
      "5\tValidation loss: 1.461506\tBest loss: 1.457405\tAccuracy: 51.33%\n",
      "6\tValidation loss: 1.671328\tBest loss: 1.457405\tAccuracy: 44.00%\n",
      "7\tValidation loss: 1.822386\tBest loss: 1.457405\tAccuracy: 44.67%\n",
      "8\tValidation loss: 1.759439\tBest loss: 1.457405\tAccuracy: 46.00%\n",
      "9\tValidation loss: 1.411399\tBest loss: 1.411399\tAccuracy: 45.33%\n",
      "10\tValidation loss: 1.835194\tBest loss: 1.411399\tAccuracy: 40.67%\n",
      "11\tValidation loss: 1.877978\tBest loss: 1.411399\tAccuracy: 39.33%\n",
      "12\tValidation loss: 1.648495\tBest loss: 1.411399\tAccuracy: 39.33%\n",
      "13\tValidation loss: 1.927988\tBest loss: 1.411399\tAccuracy: 44.00%\n",
      "14\tValidation loss: 1.646698\tBest loss: 1.411399\tAccuracy: 44.67%\n",
      "15\tValidation loss: 1.603787\tBest loss: 1.411399\tAccuracy: 32.00%\n",
      "16\tValidation loss: 1.187405\tBest loss: 1.187405\tAccuracy: 52.00%\n",
      "17\tValidation loss: 1.574231\tBest loss: 1.187405\tAccuracy: 47.33%\n",
      "18\tValidation loss: 1.502473\tBest loss: 1.187405\tAccuracy: 42.00%\n",
      "19\tValidation loss: 1.580502\tBest loss: 1.187405\tAccuracy: 42.67%\n",
      "20\tValidation loss: 1.842323\tBest loss: 1.187405\tAccuracy: 42.00%\n",
      "21\tValidation loss: 1.573698\tBest loss: 1.187405\tAccuracy: 40.00%\n",
      "22\tValidation loss: 1.349187\tBest loss: 1.187405\tAccuracy: 41.33%\n",
      "23\tValidation loss: 1.378534\tBest loss: 1.187405\tAccuracy: 42.00%\n",
      "24\tValidation loss: 1.610882\tBest loss: 1.187405\tAccuracy: 42.00%\n",
      "25\tValidation loss: 1.735742\tBest loss: 1.187405\tAccuracy: 46.00%\n",
      "26\tValidation loss: 1.318292\tBest loss: 1.187405\tAccuracy: 49.33%\n",
      "27\tValidation loss: 1.601814\tBest loss: 1.187405\tAccuracy: 47.33%\n",
      "28\tValidation loss: 1.704836\tBest loss: 1.187405\tAccuracy: 29.33%\n",
      "29\tValidation loss: 1.459034\tBest loss: 1.187405\tAccuracy: 48.67%\n",
      "30\tValidation loss: 1.261706\tBest loss: 1.187405\tAccuracy: 49.33%\n",
      "31\tValidation loss: 1.405645\tBest loss: 1.187405\tAccuracy: 44.67%\n",
      "32\tValidation loss: 1.732622\tBest loss: 1.187405\tAccuracy: 37.33%\n",
      "33\tValidation loss: 1.656071\tBest loss: 1.187405\tAccuracy: 38.67%\n",
      "34\tValidation loss: 2.565177\tBest loss: 1.187405\tAccuracy: 30.00%\n",
      "35\tValidation loss: 2.233624\tBest loss: 1.187405\tAccuracy: 38.00%\n",
      "36\tValidation loss: 1.429016\tBest loss: 1.187405\tAccuracy: 50.67%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ch11/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 47.99%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"ch11/my_mnist_model_0_to_4.ckpt\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"ch11/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"ch11/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n",
      "(?, 100)\n",
      "(?, 200)\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name='X')\n",
    "# tensorを二つに分割する\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "\n",
    "# 二つの数値が等しければ１、異なれば０のラベルをつける\n",
    "y =tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, activation=tf.nn.elu, initializer=he_int):\n",
    "# variable_scope で変数を階層化\n",
    "    with tf.variable_scope(name, 'dnn'):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation, \n",
    "                                     kernel_initializer=initializer, \n",
    "                                     name='hidden%d' % (layer+1))\n",
    "        return inputs\n",
    "\n",
    "\n",
    "dnn1 = dnn(X1, name='DNN_A')\n",
    "dnn2 = dnn(X2, name='DNN_B')\n",
    "\n",
    "# 二つのネットワークの出力をまとめる\n",
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)\n",
    "\n",
    "print(dnn1.shape)\n",
    "print(dnn2.shape)\n",
    "print(dnn_outputs.shape)\n",
    "\n",
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)\n",
    "\n",
    "# tf.greater_equal で二つの値のうち、大きいほうを返す\n",
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
    "y_as_float = tf.cast(y, tf.float32)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "\n",
    "X_test = X_test\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 784) float32\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))\n",
    "\n",
    "print(X_batch.shape, X_batch.dtype)\n",
    "print(y_batch)\n",
    "\n",
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.69176936\n",
      "0 Test accuracy: 0.5244\n",
      "1 Train loss: 0.57125527\n",
      "2 Train loss: 0.55620176\n",
      "3 Train loss: 0.5481904\n",
      "4 Train loss: 0.45125982\n",
      "5 Train loss: 0.43640688\n",
      "5 Test accuracy: 0.8018\n",
      "6 Train loss: 0.42479438\n",
      "7 Train loss: 0.33413723\n",
      "8 Train loss: 0.4036409\n",
      "9 Train loss: 0.35863727\n",
      "10 Train loss: 0.3600116\n",
      "10 Test accuracy: 0.8563\n",
      "11 Train loss: 0.30332038\n",
      "12 Train loss: 0.28044146\n",
      "13 Train loss: 0.25412467\n",
      "14 Train loss: 0.27061003\n",
      "15 Train loss: 0.2567548\n",
      "15 Test accuracy: 0.8989\n",
      "16 Train loss: 0.26206276\n",
      "17 Train loss: 0.2399947\n",
      "18 Train loss: 0.16286466\n",
      "19 Train loss: 0.19637285\n",
      "20 Train loss: 0.1819351\n",
      "20 Test accuracy: 0.9201\n",
      "21 Train loss: 0.17858937\n",
      "22 Train loss: 0.17267565\n",
      "23 Train loss: 0.17332488\n",
      "24 Train loss: 0.18316536\n",
      "25 Train loss: 0.16704723\n",
      "25 Test accuracy: 0.9382\n",
      "26 Train loss: 0.13938135\n",
      "27 Train loss: 0.13096334\n",
      "28 Train loss: 0.12862495\n",
      "29 Train loss: 0.09602224\n",
      "30 Train loss: 0.0830857\n",
      "30 Test accuracy: 0.95\n",
      "31 Train loss: 0.122175954\n",
      "32 Train loss: 0.08047356\n",
      "33 Train loss: 0.11402301\n",
      "34 Train loss: 0.07137043\n",
      "35 Train loss: 0.060837656\n",
      "35 Test accuracy: 0.9554\n",
      "36 Train loss: 0.11109849\n",
      "37 Train loss: 0.09321053\n",
      "38 Train loss: 0.094014876\n",
      "39 Train loss: 0.08128654\n",
      "40 Train loss: 0.09120139\n",
      "40 Test accuracy: 0.9619\n",
      "41 Train loss: 0.08521971\n",
      "42 Train loss: 0.15020034\n",
      "43 Train loss: 0.08446554\n",
      "44 Train loss: 0.11555725\n",
      "45 Train loss: 0.067212015\n",
      "45 Test accuracy: 0.966\n",
      "46 Train loss: 0.04431092\n",
      "47 Train loss: 0.057742257\n",
      "48 Train loss: 0.05015404\n",
      "49 Train loss: 0.048063554\n",
      "50 Train loss: 0.09680209\n",
      "50 Test accuracy: 0.9674\n",
      "51 Train loss: 0.08302145\n",
      "52 Train loss: 0.04033704\n",
      "53 Train loss: 0.06952929\n",
      "54 Train loss: 0.05544297\n",
      "55 Train loss: 0.06524525\n",
      "55 Test accuracy: 0.968\n",
      "56 Train loss: 0.05605178\n",
      "57 Train loss: 0.06862112\n",
      "58 Train loss: 0.045290437\n",
      "59 Train loss: 0.06271447\n",
      "60 Train loss: 0.048653662\n",
      "60 Test accuracy: 0.9705\n",
      "61 Train loss: 0.06778813\n",
      "62 Train loss: 0.04788295\n",
      "63 Train loss: 0.055046532\n",
      "64 Train loss: 0.057607263\n",
      "65 Train loss: 0.05385564\n",
      "65 Test accuracy: 0.9725\n",
      "66 Train loss: 0.057033744\n",
      "67 Train loss: 0.08766825\n",
      "68 Train loss: 0.026241684\n",
      "69 Train loss: 0.030468985\n",
      "70 Train loss: 0.04948271\n",
      "70 Test accuracy: 0.9727\n",
      "71 Train loss: 0.062574655\n",
      "72 Train loss: 0.06353787\n",
      "73 Train loss: 0.028620858\n",
      "74 Train loss: 0.03160015\n",
      "75 Train loss: 0.035966374\n",
      "75 Test accuracy: 0.9762\n",
      "76 Train loss: 0.03613718\n",
      "77 Train loss: 0.024116686\n",
      "78 Train loss: 0.030807257\n",
      "79 Train loss: 0.015125508\n",
      "80 Train loss: 0.039221648\n",
      "80 Test accuracy: 0.9756\n",
      "81 Train loss: 0.03744566\n",
      "82 Train loss: 0.048500054\n",
      "83 Train loss: 0.024903614\n",
      "84 Train loss: 0.04700904\n",
      "85 Train loss: 0.04291397\n",
      "85 Test accuracy: 0.9782\n",
      "86 Train loss: 0.03346673\n",
      "87 Train loss: 0.025835536\n",
      "88 Train loss: 0.032392405\n",
      "89 Train loss: 0.030652061\n",
      "90 Train loss: 0.03256373\n",
      "90 Test accuracy: 0.9786\n",
      "91 Train loss: 0.02370237\n",
      "92 Train loss: 0.028145416\n",
      "93 Train loss: 0.02080139\n",
      "94 Train loss: 0.026045678\n",
      "95 Train loss: 0.015703067\n",
      "95 Test accuracy: 0.9802\n",
      "96 Train loss: 0.020406682\n",
      "97 Train loss: 0.015779637\n",
      "98 Train loss: 0.0380024\n",
      "99 Train loss: 0.024282467\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X:X_test1, y:y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "            \n",
    "        save_path = saver.save(sess, 'ch11/my_digit_comparison_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
